---
title: "Building Severity Index with R"
author: "Edouard Legoupil"
date: "2019-11-28"
categories:
  - Indicator
  - Composite
  - Severity
tags:
  - UNHCR
  - Edouard-Legoupil
---

<script src="/rmarkdown-libs/kePrint/kePrint.js"></script>


<p>In this tutorial, We will see how to build a <strong>severity index</strong> using a key informant interview dataset. Building such index, also called <strong>composite indicator</strong>, is among the regular and expected task of any humanitarian data analyst.The objective is to summarize information into a simple indicator that can reduce information overflow. <strong>Severity index</strong> informs the sectoral and inter-sectoral discussions taking place as part of the Humanitarian Needs Overview (HNO) process, in particular facilitating a comparison of needs across geographic areas.
<!--MORE--></p>
<p>An approach to follow for this is described in the <em>Annex1: Technical Guidance</em> of the 2014 <a href="https://www.humanitarianresponse.info/sites/www.humanitarianresponse.info/files/documents/files/Comparison%20Tool%20Guidance%202015%20FINAL-EN.pdf#page=12">GUIDANCE: Humanitarian Needs Comparison Tool</a>. Those guidance were provided with a <a href="https://www.humanitarianresponse.info/sites/www.humanitarianresponse.info/files/documents/files/Nigeria_HNO_Comparison_Tool_v0.1.zip">complex and heavy excel tool</a> that includes many macros script and <a href="https://www.humanitarianresponse.info/sites/www.humanitarianresponse.info/files/documents/files/Comparison%20Tool%20Technical%20Instructions.pdf">13 pages of narrative explanation</a> to explain how it works.</p>
<p>Rather than using script through complicated <em>Macros</em>, using R is a good and easier alternative. Build those severity index though a documented and linear process brings more credibility in the severity index generation as the analysis becomes de facto entirely reproducible (as a difference with complex excel spreadsheet with macro formula spread around multiple worksheet). Existing packages such as <a href="http://complexity.stat.unipd.it/system/files/Vidoli_Fusco.pdf">R Compind package</a> for composite indicators have already been developed and released as open source and therefore ready to be leveraged without any software procurement. The analysis below is fully reproducible and all code, including a summary power point, is accessible <a href="https://github.com/protection-cluster/severity-index-bahamas">here</a></p>
<p>As the analysis workflow shall remain the same, anyone can take and study this tutorial source code, then change the dataset behind with their own in order to re-use it.</p>
<div id="why-do-you-need-statistical-analysis-for-indicator-composition" class="section level2">
<h2>Why do you need statistical analysis for indicator composition?</h2>
<p>When developing severity index, each steps comes with methodological questions:</p>
<ul>
<li>How to select and organised indicators?<br />
</li>
<li>How to calculate and reshape them?</li>
<li>How to assemble them together (aggregation and weighting).</li>
</ul>
<p>The 2014 guidance advise to determine the weight of each sub-indicator through (often unreachable…) expert consensus. Such consensus, when obtained, usually takes (very long…) consultation and consideration with concerned stakeholders that are most of the time not statisticians themselves and will provide mostly guesstimates for each weight. This approach in the academic world is known as a <strong>budget allocation process</strong> (BAP), where set of chosen decision makers (e.g. a panel of experts) is given ‘n’ points to distribute to the indicators, or groups of indicators (e.g. dimensions), and then an average of the experts’ choices is used. An alternative to BAP, called “<a href="https://unhcr-mena.github.io/analytic-hierarchy-process">analytic hierarchy process - AHP</a>”, is collect each expert opinion in a structured way through pairwise comparison. A rule of thumb for any expert judgement process is to have fewer than 10 indicators so that the approach is optimally executed cognitively, as such most of the time, they are not appropriate for the development of humanitarian severity index.</p>
<p>Arithmetic and geometric aggregation with equal weighting are originally also suggested in the guidance may also comes with assumption of compensability (i.e. Perfect substituability – compensates bad performance in one aspect with good performance in another) that are often not verified. Such points have been largely discussed in literature (see <a href="https://www.acaps.org/sites/acaps/files/resources/files/acaps_technical_note_severity_measures_aug_2016_0.pdf">Benini, Aldo (2016). Severity measures in humanitarian needs assessments - Purpose,measurement, integration. Technical note - 8 August 2016 -. Geneva, Assessment CapacitiesProject (ACAPS)</a>).</p>
<p>The <a href="https://www.oecd.org/sdd/42495745.pdf">OECD handbook for composite Indicators calculation</a> also taught through the <a href="https://goo.gl/oULGgt">European Joint Research Training on Composite Indicators</a> present a series of alternatives that can inform / help confirming or triangulate “expert-judgement” . As we will see below, some statistical method are easily accessible for R users in order to get indicators weights.</p>
</div>
<div id="installing-packages" class="section level2">
<h2>Installing packages</h2>
<p>To get started, if you don’t have them already, the following packages are necessary.</p>
<pre class="r"><code>## This function will retrieve the packae if they are not yet installed.
using &lt;- function(...) {
    libs &lt;- unlist(list(...))
    req &lt;- unlist(lapply(libs,require,character.only = TRUE))
    need &lt;- libs[req == FALSE]
    if (length(need) &gt; 0) { 
        install.packages(need)
        lapply(need,require,character.only = TRUE)
    }
}

## Getting all necessary package
using(&quot;readr&quot;,&quot;readxl&quot;,&quot;dplyr&quot;,&quot;ggplot2&quot;,&quot;corrplot&quot;,&quot;psych&quot;,&quot;bbplot&quot;,
      &quot;scales&quot;,&quot;ggiraph&quot;,&quot;ggrepel&quot;,&quot;Compind&quot;,&quot;ggcorrplot&quot;,&quot;kableExtra&quot;,
      &quot;reshape2&quot;,&quot;qgraph&quot;, &quot;sf&quot;,&quot;cartography&quot;,
      &quot;raster&quot;, &quot;dismo&quot;, &quot;xlsx&quot;, &quot;rgdal&quot;)</code></pre>
<pre><code>## Warning in fun(libname, pkgname): rgeos: versions of GEOS runtime 3.8.0-CAPI-1.13.1
## and GEOS at installation 3.7.1-CAPI-1.11.1differ</code></pre>
<pre class="r"><code>rm(using)




# This small function is used to have nicely left align text within 
# charts produced with ggplot2
left_align &lt;- function(plot_name, pieces){
  grob &lt;- ggplot2::ggplotGrob(plot_name)
  n &lt;- length(pieces)
  grob$layout$l[grob$layout$name %in% pieces] &lt;- 2
  return(grob)
}

unhcr_style &lt;- function() {
  font &lt;- &quot;Lato&quot;
  ggplot2::theme(
    
#This sets the font, size, type and colour of text for the chart&#39;s title
  plot.title = ggplot2::element_text(family = font, size = 20,
                                     face = &quot;bold&quot;, color = &quot;#222222&quot;),

# This sets the font, size, type and colour of text for the chart&#39;s subtitle,
# as well as setting a margin between the title and the subtitle
  plot.subtitle = ggplot2::element_text(family = font, size = 16, 
                                        margin = ggplot2::margin(9,0,9,0)),
  plot.caption = ggplot2::element_blank(),

# This sets the position and alignment of the legend, removes a title 
# and backround for it and sets the requirements for any text within the legend.
# The legend may often need some more manual tweaking when it comes to its exact
# position based on the plot coordinates.
  legend.position = &quot;top&quot;,
  legend.text.align = 0,
  legend.background = ggplot2::element_blank(),
  legend.title = ggplot2::element_blank(),
  legend.key = ggplot2::element_blank(),
  legend.text = ggplot2::element_text(family = font, size = 13, color = &quot;#222222&quot;),

# This sets the text font, size and colour for the axis test, as well as setting
# the margins and removes lines and ticks. In some cases, axis lines and axis 
# ticks are things we would want to have in the chart
  axis.title = ggplot2::element_blank(),
  axis.text = ggplot2::element_text(family = font, size = 13, color = &quot;#222222&quot;),
  axis.text.x = ggplot2::element_text(margin = ggplot2::margin(5, b = 10)),
  axis.ticks = ggplot2::element_blank(),
  axis.line = ggplot2::element_blank(),

# This removes all minor gridlines and adds major y gridlines. 
# In many cases you will want to change this to remove y gridlines and add x gridlines. 
  panel.grid.minor = ggplot2::element_blank(),
  panel.grid.major.y = ggplot2::element_line(color = &quot;#cbcbcb&quot;),
  panel.grid.major.x = ggplot2::element_blank(),

# This sets the panel background as blank, removing the standard grey
# ggplot background colour from the plot
  panel.background = ggplot2::element_blank(),

# This sets the panel background for facet-wrapped plots to white, 
# removing the standard grey ggplot background colour and sets the title size 
# of the facet-wrap title to font size 22
  strip.background = ggplot2::element_rect(fill = &quot;white&quot;),
  strip.text = ggplot2::element_text(size  = 13,  hjust = 0)
  )
}</code></pre>
</div>
<div id="data-set---key-informant-interview-for-hurricane-dorian-bahamas" class="section level2">
<h2>Data set - Key Informant Interview for Hurricane Dorian, Bahamas</h2>
<p>This dataset was collected at the beginning of November by IOM in the Bahamas. It covers Aback Island which has been severely hit by Hurricane Dorian on 1:3 September.</p>
<p>The dataset has been publicly released and is <a href="https://data.humdata.org/dataset/the-bahamas-hurricane-dorian-site-assessment-round-3-november-2019#">available through HDX here</a>. An <a href="https://displacement.iom.int/reports/bahamas-%E2%80%94-hurricane-dorian-%E2%80%93-site-assessment-%E2%80%93-round-3-november-2019?close=true">initial report</a> has been produced.</p>
<pre class="r"><code>data &lt;- read_excel(&quot;DTM R3 DB Great-Little Abaco MSLA V4.xlsx&quot;, sheet = &quot;BD&quot;)


## For some field the value &#39;9999&#39; has been entered - for the sake of analysis, 
# we will assume that this correspond to answers &#39;no&#39; - 0
# to replace multiple values in a data frame, looping through all columns might help.
for (i in seq_along(data)) {
    data[[i]][data[[i]] == &quot;9999&quot;] &lt;- &quot;0&quot;
}
# This has converted numerci to character - let&#39;s bring them back to numeric
data[, c(3:ncol(data))] &lt;- sapply(data[, c(3:ncol(data))], as.numeric)</code></pre>
<pre><code>## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion</code></pre>
<pre class="r"><code>## Point of control
#View(data[ ,c(&quot;J_107_VisitInmigration&quot;)])</code></pre>
<p>The data includes 188 variables and 23 records, each records corresponding to a location in Abaco.</p>
</div>
<div id="building-the-theoretical-framework." class="section level2">
<h2>Building the theoretical framework.</h2>
<p>The original dataset already includes a data dictionary. The initial step is add more column to this dictionary in order to record the Severity theoretical framework.</p>
<p>A simple approach here is to regroup indicators by topics / composite dimensions. A composite dimension shall be apprehended as a <em>construct</em> i.e. a defined idea of sectoral severity that can only be measured through a number of simpler elements. A good exercise then is to try to give a definition of the type of sectoral severity that is looked at: The definition of the concept should give a clear sense of what is being measured by the composite index. It should be noted that not all indicators shall be blindly included: common sense shall be used to confirm that selected sub-indicators are indeed constructive or reflective of the actual dimension ‘construct’.</p>
<p>Looking at the spreadsheet from HDX, we can see that the data structure has already been reshaped / ‘hot coded’ as what is called a <a href="https://en.wikipedia.org/wiki/Dummy_variable_(statistics)">dummy variable</a> so that each variable that includes modalities (categorical: i.e select_one with more than 2 modalities or select_multiple) is split into as many variable as there are modalities.</p>
<p>Single questions could then be extracted and had to be renamed both in terms of short code <code>qid</code> shortened Label for good appearance on charts <code>qlabel</code>. as a rule of thumb, a good variable name shall be self-explanatory (avoid q1, q2 , etc.) and very short - not more than 10 characters - A label should not be more than 80 characters and even less if possible.</p>
<p>Then we need to mention how the variable shall be used for the composite calculation, i.e. either “binary”, “value” or “scored” as we will see later.</p>
<pre class="r"><code>### Need to implemet manually the analysis framework in the data dictionnary in order 
# to get the calculation
dico &lt;- read_excel(&quot;DTM R3 DB Great-Little Abaco MSLA V4.xlsx&quot;, sheet = &quot;CatPregs&quot;)


## Checking the dico matche with dataframe --
dico1 &lt;- as.data.frame(names(data))
names(dico1)[1] &lt;- &quot;cod_pregunta&quot;
dico &lt;- plyr::join(x = dico1 , y = dico , by = &quot;cod_pregunta&quot;, type = &quot;left&quot; )
# Adding variable label in data frame
for (i in 1:nrow(data)) { attributes(data)$variable.labels[ i] &lt;- as.character(dico[ i, c(&quot;label&quot;)]) }


rm(i, dico1)

## Now creating score variable and applying same direction to all of them!

#levels(as.factor(dico$Calculation))
#labels(dico)
subindicator &lt;- dico[ dico$Calculation %in% c(&quot;binary&quot;, &quot;value&quot;, &quot;scored&quot;), ]
subindicator.unique &lt;- as.data.frame( unique(subindicator[ ,c(&quot;qid&quot;,  &quot;question&quot;,
                                                              &quot;qtype&quot; ,
                                                              &quot;Calculation&quot;, &quot;Dimension&quot;,
                                                              &quot;qlabel&quot;,&quot;justification&quot;,
                                                              &quot;Polarity&quot;   )]))

## Display the table
subindicator.unique2 &lt;- as.data.frame( unique(subindicator[ ,c( &quot;Dimension&quot;,&quot;qlabel&quot;,
                                                                &quot;qtype&quot; , &quot;Calculation&quot;,  
                                                                &quot;Polarity&quot;   )]))
row.names(subindicator.unique2) &lt;- NULL


##  Frame with all dimensions
dimensions &lt;- as.data.frame( unique(subindicator[ ,c( &quot;Dimension&quot; )]))
names(dimensions)[1] &lt;- &quot;Dimension&quot;
## Get first Protection
i &lt;- 2
  ## looping around dimensions
this.dimension &lt;- as.character(dimensions[i,1])
  
subindicator.unique3 &lt;- as.data.frame( subindicator.unique2[ subindicator.unique2$Dimension == this.dimension ,
                                                  c( &quot;qlabel&quot;,  &quot;Calculation&quot;,
                                                                &quot;Polarity&quot;   )])
row.names(subindicator.unique3) &lt;- NULL  

knitr::kable(subindicator.unique3, caption = &quot;Theoretical Framework&quot;) %&gt;%
           kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;bordered&quot;,
                                               &quot;condensed&quot;, &quot;responsive&quot;),
                         font_size = 9)</code></pre>
<table class="table table-striped table-bordered table-condensed table-responsive" style="font-size: 9px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-2">Table 1: </span>Theoretical Framework
</caption>
<thead>
<tr>
<th style="text-align:left;">
qlabel
</th>
<th style="text-align:left;">
Calculation
</th>
<th style="text-align:left;">
Polarity
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Concerning hierarchy of unmet needs
</td>
<td style="text-align:left;">
scored
</td>
<td style="text-align:left;">
Negative (the higher score, the more severe)
</td>
</tr>
<tr>
<td style="text-align:left;">
Community Relation Level
</td>
<td style="text-align:left;">
scored
</td>
<td style="text-align:left;">
Negative (the higher score, the more severe)
</td>
</tr>
<tr>
<td style="text-align:left;">
Relation with Host Community
</td>
<td style="text-align:left;">
scored
</td>
<td style="text-align:left;">
Negative (the higher score, the more severe)
</td>
</tr>
<tr>
<td style="text-align:left;">
Women Good safety
</td>
<td style="text-align:left;">
scored
</td>
<td style="text-align:left;">
Positive (the higher score, the less severe)
</td>
</tr>
<tr>
<td style="text-align:left;">
Men Good Safety
</td>
<td style="text-align:left;">
scored
</td>
<td style="text-align:left;">
Positive (the higher score, the less severe)
</td>
</tr>
<tr>
<td style="text-align:left;">
Child Good Safety
</td>
<td style="text-align:left;">
scored
</td>
<td style="text-align:left;">
Positive (the higher score, the less severe)
</td>
</tr>
<tr>
<td style="text-align:left;">
Reported Security Incident
</td>
<td style="text-align:left;">
binary
</td>
<td style="text-align:left;">
Negative (the higher score, the more severe)
</td>
</tr>
<tr>
<td style="text-align:left;">
Positive Safe/Recreational Places
</td>
<td style="text-align:left;">
binary
</td>
<td style="text-align:left;">
Positive (the higher score, the less severe)
</td>
</tr>
<tr>
<td style="text-align:left;">
Positive Diversity of Information source
</td>
<td style="text-align:left;">
scored
</td>
<td style="text-align:left;">
Positive (the higher score, the less severe)
</td>
</tr>
</tbody>
</table>
</div>
<div id="calculating-each-sub-indicator-value" class="section level2">
<h2>Calculating each sub-indicator value</h2>
<p>The survey has mostly responses of types: ‘select_one’ and ‘select_multiple’. Indeed, collecting reliable numeric value is a <a href="https://twitter.com/edouard_lgp/status/1151862004413607939">well-knwon limitation of key-informant based survey</a>.</p>
<p>Different calculations were used depending on the type of questions:</p>
<ul>
<li><p>For binary questions, negative questions receive a score of 1 when answered positively, such as answering <code>no</code>, and the score was lowered towards 0 with each negative response given. This will require as we will see below a normalization based on ranking in order to ensure that geometric means can be done. Ridit analysis essentially transforms ordinal data to a probability scale (one could call it a virtual continuous scale). The term actually stands for relative to an identified distribution integral transformation</p></li>
<li><p>Some <code>select_one</code> response choices is ordinal data with an imputed ordered response, where the max score is given either to the best or worst possible choice.</p></li>
<li><p>Other <code>select_multiple</code> questions might have many dummy variables corresponded to the question and the sum of these scores represented the highest score possible. Other <code>select_multiple</code> response choices are discrete choice data with nominal responses; each answer in the <code>select_multiple</code> are weighted according to importance or criticality.</p></li>
</ul>
<pre class="r"><code>## Creating scores for each of those indicator ###########
## Num col where indic will start to be appended
numcol1 &lt;- ncol(data) + 1
numcol &lt;- ncol(data)

## looping around each new sub indicator to create
for (j in 1:nrow(subindicator.unique)) {

    # j &lt;- 2
    this.indicator.comp &lt;- as.character(subindicator.unique[ j, c(&quot;Calculation&quot;)])
    this.indicator.type &lt;- as.character(subindicator.unique[ j, c(&quot;qtype&quot;)])
    this.indicator.name &lt;- as.character(subindicator.unique[ j, c(&quot;qid&quot;)])
    this.indicator.label &lt;- as.character(subindicator.unique[ j, c(&quot;qlabel&quot;)])
    this.indicator.question &lt;- as.character(subindicator.unique[ j, c(&quot;question&quot;)])
    this.indicator.Polarity &lt;- as.character(subindicator.unique[ j, c(&quot;Polarity&quot;)])
    this.indicator.Dimension &lt;- as.character(subindicator.unique[ j, c(&quot;Dimension&quot;)])

    ## let&#39;s add a variable in the data frame and give it the indic name
    numcol &lt;- numcol + 1
    data[ , numcol] &lt;- &quot;&quot;
    names(data)[numcol] &lt;- this.indicator.name
    attributes(data)$variable.labels[numcol] &lt;- this.indicator.label

    ## Display where we are in the console - always usefull to debug!
    ## Take the # when playing with the script
    #cat(paste0(&quot;\n\n===  Indicator: &quot;, j, &quot;-&quot;,this.indicator.Dimension ,
    #           &quot;-&quot;,this.indicator.label ,&quot;\n&quot;,this.indicator.comp ,
    #           &quot;-&quot;,this.indicator.Polarity ,&quot;\n&quot;))

    ## Now accounting for 2 distinct cases to get my calculation
    ## &quot;binary&quot; / &quot;value&quot; or &quot;scored&quot;

      ## If it&#39;s a score, we need to sum up all scores for that questions  #####
      # - independently of wether it&#39;s a select_one or select_multiple -
      if (this.indicator.comp == &quot;scored&quot; ) {
      ## get the correponding value var
        this.value.var &lt;- as.character(subindicator[ subindicator$question == this.indicator.question,
                                                     c(&quot;cod_pregunta&quot;) ])
        this.subset &lt;- data[ , this.value.var]

        ## Now get an apply the coeffcient
        this.subsetscore &lt;- t(as.data.frame(subindicator[ subindicator$cod_pregunta %in% this.value.var,
                                                          c(&quot;scoremodality&quot;) ]))

      ## multiply data frame by a vector
      this.subset3 &lt;- data.frame(mapply(`*`,this.subset, this.subsetscore, SIMPLIFY = FALSE))
      #str(this.subset3)
      # this.subset4 &lt;- cbind(this.subset3,  rowSums(this.subset3, na.rm = TRUE))

      #cat(paste0(&quot;Calculation of Indicator: &quot;, j, &quot;\n&quot;))
      ## Get the sum of the row
      data[ , numcol] &lt;- rowSums(this.subset3, na.rm = TRUE)
    }
     

    ## If it&#39;s a  value, we just take the value of that binary  #####-
    if (this.indicator.comp %in% c(&quot;value&quot;)) {
      ## get the correponding value var
      this.value.var &lt;- as.character(dico[ dico$question == this.indicator.question,
                                           c(&quot;cod_pregunta&quot;) ])
      ## Apply the value
      #cat(paste0(&quot;Calculation of Indicator: &quot;, j, &quot;\n&quot;))
      data[ , numcol] &lt;- data[ , this.value.var]
    }
    
    ## If it&#39;s a binary , we add 1 to avoid zero value #####-
    if (this.indicator.comp %in% c(&quot;binary&quot;)) {
      ## get the correponding value var
      this.value.var &lt;- as.character(dico[ dico$question == this.indicator.question,
                                           c(&quot;cod_pregunta&quot;) ])
      
      #cat(paste0(&quot;Calculation of Indicator: &quot;, j, &quot;\n&quot;))
      
      ## Apply the value + 1 to avoid zero value
      data[ , numcol] &lt;- data[ , this.value.var] + 1
      
      ## Ridit transformation
      # data[ , numcol] &lt;- (cumsum(data[ , this.value.var]) - .5 *
      #                       data[ , this.value.var]) /
      #                          sum(data[ , this.value.var])
      
    }
      
    ## clean
    rm(this.indicator.comp, this.indicator.type, this.indicator.name,
       this.indicator.label, this.indicator.question, this.indicator.Polarity ,
       this.indicator.Dimension, this.subset, this.subset3, this.subsetscore, this.value.var )
}</code></pre>
<div id="sub-indicator-polarity-data-normalization" class="section level3">
<h3>Sub-Indicator Polarity &amp; Data Normalization,</h3>
<p>We can now isolate our new numeric and scaled indicators within a matrix as this object type will be required for the rest of the calculations.</p>
<p>First, we need to eliminate indicators with poor discrimination capacity, i.e. when all values are the sames.</p>
<pre class="r"><code>## Double Checking results
#View(data[ , numcol1:ncol(data)])
indic &lt;- data[ , numcol1:ncol(data)]

### Remove var when standard deviation is 0
indic2 &lt;- indic[, sapply(indic, function(x) { sd(x) != 0} )]

## For some field, we still have indic with zero value - 
## This a data quality issue that shows that some modalities were missing 
# for select_one or select_multiple
### We assume that this account for not severe situation - i.e. replaced by one
for (i in seq_along(indic2)) {
    indic2[[i]][indic2[[i]] == &quot;0&quot;] &lt;- &quot;1&quot;
}
# This has converted from numeric to character - let&#39;s bring them back to numeric
indic2 &lt;- sapply(indic2, as.integer)

## Refresh my dictionnary of indic
subindicator.unique2 &lt;- subindicator.unique[ subindicator.unique$qid
                                             %in% names(as.data.frame(indic2)), ]

## Transform this object as a matrix and inject location name as row.names
indic.matrix &lt;- as.matrix(indic2)
row.names(indic.matrix) &lt;- data$C_101_name</code></pre>
<p>The <strong>polarity</strong> of a sub-indicator is the sign of the relationship between the indicator and the phenomenon to be measured (e.g., in a well-being index, “GDP per capita” has ‘positive’ polarity and “Unemployment rate” has ‘negative’ polarity). In this case, we have 2 options for such directional adjustments:</p>
<ul>
<li>“Negative (the higher score, the more severe)”</li>
<li>“Positive (the higher score, the less severe)”</li>
</ul>
<p>This component is accounted for during the normalization process below.</p>
<p><strong>Data Normalization</strong> allows for Adjustments of distribution (similar range of variation) and scale (common scale) of sub-indicators that may reflect different units of measurement and different ranges of variation.</p>
<p>Due to the structure of the indicators, distinct approaches of normalization shall be considered in order to avoid having zero value that would create issues for geometric means aggregation. Different normalization methods are available through the function <code>normalise_ci</code> and lead to different results:</p>
<ul>
<li><p>A z-score approach <code>method = 1</code> (Imposes a distribution with mean zero and variance 1). Standardized scores which are below average will become negative, implying that further geometric aggregation will be prevented.</p></li>
<li><p>A min-max approach <code>method = 2</code> (same range of variation [0,1]) but not same variance).This method is very sensitive to extreme values/outliers</p></li>
<li><p>A ranking method <code>method = 3</code>. Scores are replaced by ranks – e.g. the highest score receives the first ranking position (rank 1).</p></li>
</ul>
<pre class="r"><code>## Retrieve polarity from dictionnary
for (i in 1:nrow(subindicator.unique2)) {
  if (subindicator.unique2[ i, c(&quot;Polarity&quot;)] == &quot;Negative (the higher score, the more severe)&quot;)  
    {subindicator.unique2[ i, c(&quot;polarity&quot;)]  &lt;- &quot;NEG&quot;} else 
    {subindicator.unique2[ i, c(&quot;polarity&quot;)]  &lt;- &quot;POS&quot;}
 }
subindicator.unique2$dir &lt;- 1

## Case 1
subindicator.scored &lt;- subindicator.unique2[ subindicator.unique2$Calculation == &quot;scored&quot;, ]
var.scored &lt;- as.character(subindicator.scored[ , c(&quot;qid&quot;) ])
indic.matrix.scored &lt;- indic.matrix[ , var.scored ]
indic.matrix.scored.obj &lt;- normalise_ci(indic.matrix.scored,
                                     c(1:ncol(indic.matrix.scored)),
                                     polarity =  as.character(subindicator.scored$polarity ),
                                     method = 3)

## Case 2
# subindicator.value &lt;- subindicator.unique2[ subindicator.unique2$Calculation == &quot;value&quot;, ]
# var.value &lt;- as.character(subindicator.value[ , c(&quot;qid&quot;) ])
# indic.matrix.value &lt;- indic.matrix[ , var.value ]
# indic.matrix.value.obj &lt;- normalise_ci(indic.matrix.value,
#                                      c(1:ncol(indic.matrix.value)),
#                                      polarity =  as.character(subindicator.value$polarity ),
#                                      method = 2)

## Case 3
subindicator.binary &lt;- subindicator.unique2[ subindicator.unique2$Calculation == &quot;binary&quot;, ]
var.binary &lt;- as.character(subindicator.binary[ , c(&quot;qid&quot;) ])
indic.matrix.binary &lt;- indic.matrix[ , var.binary ]
indic.matrix.binary.obj &lt;- normalise_ci(indic.matrix.binary,
                                     c(1:ncol(indic.matrix.binary)),
                                     polarity =  as.character(subindicator.binary$polarity ),
                                     method = 3)

## Binding this together so that we have the full normalised matrix
indic.matrix.norm &lt;- cbind(indic.matrix.scored.obj$ci_norm,
                          # indic.matrix.value.obj$ci_norm,
                           indic.matrix.binary.obj$ci_norm)

## Clean the work environment  
rm( subindicator.unique,
 #  subindicator.value, var.value , indic.matrix.value, indic.matrix.value.obj,
   subindicator.binary, var.binary , indic.matrix.binary, indic.matrix.binary.obj,
   subindicator.scored, var.scored , indic.matrix.scored, indic.matrix.scored.obj   )</code></pre>
</div>
<div id="correlation-analysis" class="section level3">
<h3>Correlation analysis</h3>
<p>The investigation of the structure of simple indicators can be done by means of correlation analysis.</p>
<p>We will check such correlation first within each dimension, using the <a href="http://www.sthda.com/english/wiki/ggcorrplot">ggcorrplot</a> package.
’</p>
<pre class="r"><code>##  Frame with all dimensions
# dimensions &lt;- as.data.frame( unique(subindicator[ ,c( &quot;Dimension&quot; )]))
# names(dimensions)[1] &lt;- &quot;Dimension&quot;

## Creating severity subindice on each dimensions with Data Envelopment analysis #####

#for (i in 1:nrow(dimensions)) {
#for (i in 1:2) {  
  # i &lt;- 2
  # ## looping around dimensions
  # this.dimension &lt;- as.character(dimensions[i,1])
  ## subset related indicator names
  this.indicators &lt;- as.character(subindicator.unique2[ subindicator.unique2$Dimension == this.dimension,
                                                        c(&quot;qid&quot;) ])
  this.indicators.label &lt;- as.character(subindicator.unique2[ subindicator.unique2$Dimension == this.dimension,
                                                        c(&quot;qlabel&quot;) ])
  ##subset matrix &amp; df
  this.indic.matrix.norm &lt;- indic.matrix[ , this.indicators]
  this.indic.df &lt;- indic2[ , this.indicators]

### Check correlation
corr.matrix &lt;- cor(this.indic.matrix.norm, method = &quot;pearson&quot;,  use = &quot;pairwise.complete.obs&quot;)
  
  
## replace with Label inside the matrix
corr.matrix1 &lt;- corr.matrix
rownames(corr.matrix1) &lt;- as.character(subindicator.unique2[subindicator.unique2$qid %in% rownames(corr.matrix), c(&quot;qlabel&quot;)])
colnames(corr.matrix1) &lt;- as.character(subindicator.unique2[subindicator.unique2$qid %in% colnames(corr.matrix), c(&quot;qlabel&quot;)])

plot1 &lt;- ggcorrplot(corr.matrix1 ,
                    method = &quot;circle&quot;,
                    hc.order = TRUE,
                    type = &quot;upper&quot;) +
  labs(title = paste0( &quot;Severity Indicators for &quot;,this.dimension ),
       subtitle = &quot;Identified Correlation between indicators&quot;,
       caption = &quot;Correlation level = dot size, Positive Correlation  = Red - Negative = Blue&quot;,
       x = NULL, y = NULL) +
  bbc_style() +
  theme( plot.title = element_text(size = 13),
         plot.subtitle = element_text(size = 11),
         plot.caption = element_text(size = 7, hjust = 1),
         axis.text = element_text(size = 7),
         strip.text.x = element_text(size = 7),
         axis.text.x = element_text(angle = 45, hjust = 1),
         legend.position = &quot;top&quot;,
         legend.box = &quot;horizontal&quot;,
         legend.text = element_text(size = 9),
         panel.grid.major.x = element_line(color = &quot;#cbcbcb&quot;),
         panel.grid.major.y = element_line(color = &quot;#cbcbcb&quot;))
ggpubr::ggarrange(left_align(plot1, c(&quot;subtitle&quot;, &quot;title&quot;)), ncol = 1, nrow = 1)</code></pre>
<p><img src="/post/CompositeIndicator_files/figure-html/unnamed-chunk-6-1.png" width="768" /></p>
<p>Another approach to better visualize correlation between indicators is to <a href="http://sachaepskamp.com/files/Cookbook.html#customizing-graphs">represent them through a network</a> with the <a href="https://github.com/thomasp85/ggraph">ggpraph</a> package.</p>
<pre class="r"><code>qgraph(cor(this.indic.matrix.norm),
     # shape = &quot;circle&quot;,
     # posCol = &quot;darkgreen&quot;,
     # negCol = &quot;darkred&quot;,
     # threshold = &quot;bonferroni&quot;, #The threshold argument can be used to remove edges that are not significant.
     # sampleSize = nrow(scores.this.norm),
     # graph = &quot;glasso&quot;,
       esize = 35, ## Size of node
       vsize = 6,
       vTrans = 600,
       posCol = &quot;#003399&quot;, ## Color positive correlation Dark powder blue
       negCol = &quot;#FF9933&quot;, ## Color negative correlation Deep Saffron
       alpha = 0.05,
       cut = 0.4, ## cut off value for correlation
       maximum = 1, ## cut off value for correlation
       palette = &#39;pastel&#39;, # adjusting colors
       borders = TRUE,
       details = FALSE,
       layout = &quot;spring&quot;,
       nodeNames = this.indicators.label ,
       legend.cex = 0.4,
       title = paste0(&quot;Correlations Network for severity indicators related &quot;,this.dimension ),
       line = -2,
       cex.main = 2)</code></pre>
<p><img src="/post/CompositeIndicator_files/figure-html/unnamed-chunk-7-1.png" width="960" /></p>
</div>
<div id="consistency-between-indicators" class="section level3">
<h3>Consistency between indicators</h3>
<p>Cronbach’s alpha, (or coefficient alpha), developed by Lee Cronbach in 1951, measures reliability (i.e. how well a test measures what it should: measure of the stability of test scores), or <a href="https://www.statisticshowto.datasciencecentral.com/internal-consistency/">internal consistency</a>.</p>
<p>As a rule of thumbs, a score of more than 0.7 indicates an acceptable level of consistency:</p>
<ul>
<li>A high level for alpha may mean that all indicators are highly correlated (meaning we have redundant indicators representing the same thing…).</li>
<li>A low value for alpha may mean that there are not enough indicators or that the indicators are poorly interrelated.</li>
</ul>
<p>’</p>
<pre class="r"><code>Cronbach.this &lt;- psych::alpha(this.indic.matrix.norm, check.keys = TRUE)

cat(paste0(&quot;The Cronbach Alpha measure of consistency for this combination of indicators is  &quot;, round(Cronbach.this$total$std.alpha, 2), &quot;\n.&quot; ) )</code></pre>
<pre><code>The Cronbach Alpha measure of consistency for this combination of indicators is  0.77
.</code></pre>
</div>
</div>
<div id="aggregation-weighting" class="section level2">
<h2>Aggregation &amp; Weighting</h2>
<p>For weighting, the main issue to address is related to the concept of <strong>compensability</strong>. Namely the question is to know to what extent can we accept that the high score of an indicator go to compensate the low score of another indicator? This problem of compensability is intertwined with the issue of attribution of weights for each sub-indicator in order to calculate the final aggregation.</p>
<p>We can foresee that using <em>“equal weight”</em> (all indicators account for the same in the final index) and <em>“arithmetic aggregation”</em> (all indicators are substituable) is unlikely to depict the complex issue of Humanitarian Severity and is likely to comes with the risk of misrepresenting the reality.</p>
<p>Various methods are available within the <a href="https://cran.r-project.org/web/packages/Compind/index.html">Compind package</a> are described below. This R package is also available through a <a href="https://fvidoli.shinyapps.io/compind_app/">ShinyApp</a>. We will then share the code to use them based on our example.</p>
<div id="benefit-of-the-doubt-approach-bod" class="section level3">
<h3>Benefit of the Doubt approach (BoD)</h3>
<p>This method is the application of Data Envelopment Analysis (DEA) to the field of composite indicators. It was originally proposed by Melyn and Moesen (1991) to evaluate macroeconomic performance. ACAPS has prepared an excellent note on <a href="https://www.acaps.org/sites/acaps/files/resources/files/the_use_of_data_envelopment_analysis_to_calculate_priority_scores_in_needs_assessments_july_2015.pdf">The use of data envelopment analysis to calculate priority scores in needs assessments</a>.</p>
<p>BoD approach offers several advantages:</p>
<ul>
<li><p>Weights are endogenously determined by the observed performances and benchmark is not based on theoretical bounds, but it’s a linear combination of the observed best performances.</p></li>
<li><p>Principle is easy to communicate: since we are not sure about the right weights, we look for ”benefit of the doubt” weights (such that your overall relative performance index is as high as possible).</p></li>
</ul>
<pre class="r"><code>CI_BoD_estimated = ci_bod(this.indic.matrix.norm,
                          indic_col = (1:ncol(this.indic.matrix.norm)))

ci_bod_est &lt;- as.data.frame( CI_BoD_estimated$ci_bod_est)
names(ci_bod_est) &lt;- &quot;Benef_Doubt&quot;</code></pre>
</div>
<div id="directional-benefit-of-the-doubt-d-bod" class="section level3">
<h3>Directional Benefit of the Doubt (D-BoD)</h3>
<p>Directional Benefit of the Doubt (D-BoD) model enhances non-compensatory property by introducing directional penalties in a standard BoD model in order to consider the preference structure among simple indicators. This method is described in the article <a href="https://www.aisre.it/images/old_papers/AISRE.pdf">Enhancing non compensatory composite indicators: a directional proposal</a>.</p>
<pre class="r"><code>## Endogenous weight - no zero weight --
CI_Direct_BoD_estimated &lt;-  ci_bod_dir(this.indic.matrix.norm,
                                       indic_col = (1:ncol(this.indic.matrix.norm)),
                                       dir = as.numeric(subindicator.unique2[subindicator.unique2$Dimension == this.dimension , c(&quot;dir&quot;)] ))

ci_bod_dir_est &lt;- data.frame(CI_Direct_BoD_estimated$ci_bod_dir_est)
names(ci_bod_dir_est) &lt;- &quot;Benef_Doubt_Dir&quot;</code></pre>
</div>
<div id="robust-benefit-of-the-doubt-approach-rbod" class="section level3">
<h3>Robust Benefit of the Doubt approach (RBoD)</h3>
<p>This method is the robust version of the BoD method. It is based on the concept of the expected minimum input function of order-m so “in place of looking for the lower boundary of the support of F, as was typically the case for the full-frontier (DEA or FDH), the order-m efficiency score can be viewed as the expectation of the maximal score, when compared to m units randomly drawn from the population of units presenting a greater level of simple indicators”, Daraio and Simar (2005). This method is described with more detail in the article <a href="http://sa-ijas.stat.unipd.it/sites/sa-ijas.stat.unipd.it/files/06.pdf">Robust weighted composite indicators by means of frontier methods with an application to European infrastructure endowment</a>.</p>
<pre class="r"><code>CI_RBoD_estimated &lt;-  ci_rbod(this.indic.matrix.norm,
                              indic_col = (1:ncol(this.indic.matrix.norm)),
                              M = 20,  #The number of elements in each sample.
                              B = 200) #The number of bootstap replicates.

ci_rbod_est &lt;- data.frame(CI_RBoD_estimated$ci_rbod_est)
names(ci_rbod_est) &lt;- &quot;Benef_Doubt_Rob&quot;</code></pre>
</div>
<div id="benefit-of-the-doubt-approach-bod-index-with-constraints-on-weights" class="section level3">
<h3>Benefit of the Doubt approach (BoD) index with constraints on weights</h3>
<p>This method allows for constraints (so that there is none not valued) and with a penalty as proposed by Mazziotta - Pareto (also adopted by the Italian National Institute of Statistics). This method is described in the article <a href="https://www.sciencedirect.com/science/article/abs/pii/S0377221716305938">Geometric mean quantity index numbers with Benefit-of-the-Doubt weights</a></p>
<pre class="r"><code>CI_BoD_MPI_estimated = ci_bod_constr_mpi(this.indic.matrix.norm,
                                          indic_col = (1:ncol(this.indic.matrix.norm)),
                                          up_w = 1,
                                          low_w = 0.1,
                                          penalty = &quot;POS&quot;)

ci_bod_constr_est_mpi &lt;- data.frame(CI_BoD_MPI_estimated$ci_bod_constr_est_mpi)
names(ci_bod_constr_est_mpi) &lt;- &quot;Benef_Doubt_Cons&quot;</code></pre>
</div>
<div id="factor-analysis" class="section level3">
<h3>Factor analysis</h3>
<p>This method groups together simple indicators to estimate a composite indicator that captures as much as possible of the information common to individual indicators.</p>
<pre class="r"><code>##  Doing PCA with ci_factor.R
# If method = &quot;ONE&quot; (default) the composite indicator estimated values are equal to first component scores;
# if method = &quot;ALL&quot; the composite indicator estimated values are equal to component score multiplied by its proportion variance;
# if method = &quot;CH&quot; it can be choose the number of the component to take into account.
dimfactor &lt;- ifelse(ncol(this.indic.matrix.norm) &gt; 2, 3, ncol(this.indic.matrix.norm))
CI_Factor_estimated &lt;-  ci_factor(this.indic.matrix.norm,
                                  indic_col = (1:ncol(this.indic.matrix.norm)),
                                  method = &quot;CH&quot;,  # if method = &quot;CH&quot; it can be choose the number of the component to take into account.
                                  dim = dimfactor)
ci_factor_est &lt;- data.frame( CI_Factor_estimated$ci_factor_est)
names(ci_factor_est) &lt;- &quot;Factor&quot;</code></pre>
</div>
<div id="mean-min-function-mmf" class="section level3">
<h3>Mean-Min Function (MMF)</h3>
<p>This method is an intermediate case between arithmetic mean, according to which no unbalance is penalized, and min function, according to which the penalization is maximum. It depends on two parameters that are respectively related to the intensity of penalization of unbalance (alpha) and intensity of complementarity (beta) among indicators. “An unbalance adjustment method for development indicators”</p>
<pre class="r"><code>CI_mean_min_estimated &lt;- ci_mean_min(this.indic.matrix.norm,
                                     indic_col = (1:ncol(this.indic.matrix.norm)),
                                     alpha = 0.5,  #intensity of penalization of unbalance  (alpha)
                                     beta = 1) # intensity of complementarity (beta) among indicators

ci_mean_min_est &lt;- data.frame( CI_mean_min_estimated$ci_mean_min_est)
names(ci_mean_min_est) &lt;- &quot;Mean_Min&quot;</code></pre>
</div>
<div id="geometric-aggregation" class="section level3">
<h3>Geometric aggregation</h3>
<p>This method uses the geometric mean to aggregate the single indicators and therefore allows to bypass the full compensability hypothesis using geometric mean. Two weighting criteria are possible: EQUAL: equal weighting and BOD: Benefit-of-the-Doubt weights following the Puyenbroeck and Rogge (2017) approach.</p>
<pre class="r"><code>CI_Geom_estimated = ci_geom_gen(this.indic.matrix.norm,
                                indic_col = (1:ncol(this.indic.matrix.norm)),
                                meth = &quot;EQUAL&quot;,
                                ## &quot;EQUAL&quot; = Equal weighting set, &quot;BOD&quot; = Benefit-of-the-Doubt weighting set.
                                up_w = 1,
                                low_w = 0.1,
                                bench = 1)
# Row number of the benchmark unit used to normalize the data.frame x.

ci_mean_geom_est &lt;- data.frame( CI_Geom_estimated$ci_mean_geom_est)
names(ci_mean_geom_est) &lt;- &quot;Mean_Geom&quot;</code></pre>
</div>
<div id="mazziotta-pareto-index-mpi" class="section level3">
<h3>Mazziotta-Pareto Index (MPI)</h3>
<p>This method is is a non-linear composite index method which transforms a set of individual indicators in standardized variables and summarizes them using an arithmetic mean adjusted by a “penalty” coefficient related to the variability of each unit (method of the coefficient of variation penalty).</p>
<pre class="r"><code>CI_MPI_estimated &lt;- ci_mpi(this.indic.matrix.norm,
                           indic_col = (1:ncol(this.indic.matrix.norm)),
                           penalty = &quot;NEG&quot;)  # Penalty direction; ”POS” (default) in case of increasing
#  or “positive” composite index (e.g., well-being index),
#  ”NEG” in case of decreasing or “negative” composite
#  index (e.g., poverty index).

ci_mpi_est &lt;- data.frame( CI_MPI_estimated$ci_mpi_est)
names(ci_mpi_est) &lt;- &quot;Mazziotta_Pareto&quot;</code></pre>
</div>
<div id="wroclaw-taxonomy-method" class="section level3">
<h3>Wroclaw taxonomy method</h3>
<p>This last method (also known as the dendric method), originally developed at the University of Wroclaw, is based on the distance from a theoretical unit characterized by the best performance for all indicators considered; the composite indicator is therefore based on the sum of euclidean distances from the ideal unit and normalized by a measure of variability of these distance (mean + 2*std).</p>
<pre class="r"><code>CI_wroclaw_estimated &lt;-  ci_wroclaw(this.indic.matrix.norm,
                                    indic_col = (1:ncol(this.indic.matrix.norm)))

ci_wroclaw_est &lt;- data.frame( CI_wroclaw_estimated$ci_wroclaw_est)
names(ci_wroclaw_est) &lt;- &quot;Wroclaw&quot;</code></pre>
</div>
</div>
<div id="visualise-output" class="section level2">
<h2>Visualise output</h2>
<div id="in-a-table" class="section level3">
<h3>In a table</h3>
<pre class="r"><code>this.indic.matrix.norm2 &lt;- cbind( #row.names(scores.this),
  ci_bod_est, # Benefit of the Doubt approach
  ci_rbod_est, # Robust Benefit of the Doubt approach
  ci_bod_dir_est, # Directional Robust Benefit of the Doubt approach
  ci_bod_constr_est_mpi, # Robust Benefit of the Doubt approach with constraint
  ci_factor_est, # Factor analysis  componnents
  ci_mean_geom_est, # Geometric aggregation
  ci_mean_min_est, # Mean-Min Function
  ci_mpi_est, # Mazziotta-Pareto Index
  ci_wroclaw_est) # Wroclaw taxonomy method


kable(this.indic.matrix.norm2, caption = &quot;Composite with different algorithm&quot;) %&gt;%
           kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;bordered&quot;, &quot;condensed&quot;, &quot;responsive&quot;), font_size = 9)</code></pre>
<table class="table table-striped table-bordered table-condensed table-responsive" style="font-size: 9px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-18">Table 2: </span>Composite with different algorithm
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Benef_Doubt
</th>
<th style="text-align:right;">
Benef_Doubt_Rob
</th>
<th style="text-align:right;">
Benef_Doubt_Dir
</th>
<th style="text-align:right;">
Benef_Doubt_Cons
</th>
<th style="text-align:right;">
Factor
</th>
<th style="text-align:right;">
Mean_Geom
</th>
<th style="text-align:right;">
Mean_Min
</th>
<th style="text-align:right;">
Mazziotta_Pareto
</th>
<th style="text-align:right;">
Wroclaw
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Bahama Palm Shores
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
1.264005
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
0.9797208
</td>
<td style="text-align:right;">
0.5494164
</td>
<td style="text-align:right;">
2.528472
</td>
<td style="text-align:right;">
105.32789
</td>
<td style="text-align:right;">
133.43501
</td>
<td style="text-align:right;">
0.1977937
</td>
</tr>
<tr>
<td style="text-align:left;">
Bahamas Coral Island
</td>
<td style="text-align:right;">
0.6666667
</td>
<td style="text-align:right;">
1.000000
</td>
<td style="text-align:right;">
0.6666667
</td>
<td style="text-align:right;">
0.2857143
</td>
<td style="text-align:right;">
-0.6148423
</td>
<td style="text-align:right;">
1.000000
</td>
<td style="text-align:right;">
88.84537
</td>
<td style="text-align:right;">
91.34200
</td>
<td style="text-align:right;">
0.8691159
</td>
</tr>
<tr>
<td style="text-align:left;">
Blackwood
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
1.298701
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
0.8219178
</td>
<td style="text-align:right;">
0.0986371
</td>
<td style="text-align:right;">
2.472720
</td>
<td style="text-align:right;">
104.62942
</td>
<td style="text-align:right;">
138.63625
</td>
<td style="text-align:right;">
0.5959116
</td>
</tr>
<tr>
<td style="text-align:left;">
Casuarina Point
</td>
<td style="text-align:right;">
0.9189189
</td>
<td style="text-align:right;">
1.032702
</td>
<td style="text-align:right;">
0.8500000
</td>
<td style="text-align:right;">
0.7792208
</td>
<td style="text-align:right;">
0.3141703
</td>
<td style="text-align:right;">
2.026346
</td>
<td style="text-align:right;">
102.12890
</td>
<td style="text-align:right;">
128.78828
</td>
<td style="text-align:right;">
0.4523715
</td>
</tr>
<tr>
<td style="text-align:left;">
Cedar Harbour
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
1.001669
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
0.7843137
</td>
<td style="text-align:right;">
0.0381540
</td>
<td style="text-align:right;">
2.119724
</td>
<td style="text-align:right;">
103.94474
</td>
<td style="text-align:right;">
131.98927
</td>
<td style="text-align:right;">
0.6059276
</td>
</tr>
<tr>
<td style="text-align:left;">
Central Pines
</td>
<td style="text-align:right;">
0.6666667
</td>
<td style="text-align:right;">
1.000000
</td>
<td style="text-align:right;">
0.6666667
</td>
<td style="text-align:right;">
0.3333333
</td>
<td style="text-align:right;">
-0.6180444
</td>
<td style="text-align:right;">
1.080060
</td>
<td style="text-align:right;">
89.02142
</td>
<td style="text-align:right;">
91.69567
</td>
<td style="text-align:right;">
0.8391677
</td>
</tr>
<tr>
<td style="text-align:left;">
Cherokee Sound
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
1.037165
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
0.8674699
</td>
<td style="text-align:right;">
0.2977712
</td>
<td style="text-align:right;">
2.363792
</td>
<td style="text-align:right;">
108.61128
</td>
<td style="text-align:right;">
152.76616
</td>
<td style="text-align:right;">
0.5485811
</td>
</tr>
<tr>
<td style="text-align:left;">
Cooper’s Town
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
1.005025
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
0.6122449
</td>
<td style="text-align:right;">
-0.1351963
</td>
<td style="text-align:right;">
1.851749
</td>
<td style="text-align:right;">
100.95427
</td>
<td style="text-align:right;">
135.52080
</td>
<td style="text-align:right;">
0.7046651
</td>
</tr>
<tr>
<td style="text-align:left;">
Crossing Rocks
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
1.001252
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
0.6486486
</td>
<td style="text-align:right;">
0.4742669
</td>
<td style="text-align:right;">
1.937082
</td>
<td style="text-align:right;">
100.63741
</td>
<td style="text-align:right;">
128.10570
</td>
<td style="text-align:right;">
0.6696285
</td>
</tr>
<tr>
<td style="text-align:left;">
Crown Heaven
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
1.126761
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
0.8955224
</td>
<td style="text-align:right;">
0.2913669
</td>
<td style="text-align:right;">
2.440570
</td>
<td style="text-align:right;">
107.14472
</td>
<td style="text-align:right;">
147.50475
</td>
<td style="text-align:right;">
0.5146147
</td>
</tr>
<tr>
<td style="text-align:left;">
Dundas Town
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
1.010952
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
0.4615385
</td>
<td style="text-align:right;">
-0.5514938
</td>
<td style="text-align:right;">
1.317981
</td>
<td style="text-align:right;">
93.87114
</td>
<td style="text-align:right;">
106.82388
</td>
<td style="text-align:right;">
0.7541743
</td>
</tr>
<tr>
<td style="text-align:left;">
Fire Road
</td>
<td style="text-align:right;">
0.8000000
</td>
<td style="text-align:right;">
1.000000
</td>
<td style="text-align:right;">
0.7500000
</td>
<td style="text-align:right;">
0.3703704
</td>
<td style="text-align:right;">
-0.2756593
</td>
<td style="text-align:right;">
1.360790
</td>
<td style="text-align:right;">
91.19001
</td>
<td style="text-align:right;">
96.13258
</td>
<td style="text-align:right;">
0.8155244
</td>
</tr>
<tr>
<td style="text-align:left;">
Fox Town
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
1.001252
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
0.7547170
</td>
<td style="text-align:right;">
0.3914990
</td>
<td style="text-align:right;">
2.000000
</td>
<td style="text-align:right;">
101.78035
</td>
<td style="text-align:right;">
129.74709
</td>
<td style="text-align:right;">
0.6059276
</td>
</tr>
<tr>
<td style="text-align:left;">
Great Cistern
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
1.311475
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
0.5617978
</td>
<td style="text-align:right;">
0.2907193
</td>
<td style="text-align:right;">
1.757528
</td>
<td style="text-align:right;">
94.23900
</td>
<td style="text-align:right;">
102.84506
</td>
<td style="text-align:right;">
0.6628530
</td>
</tr>
<tr>
<td style="text-align:left;">
Leisure Lee
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
1.000000
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
0.5882353
</td>
<td style="text-align:right;">
0.1610722
</td>
<td style="text-align:right;">
1.759955
</td>
<td style="text-align:right;">
99.00941
</td>
<td style="text-align:right;">
122.01924
</td>
<td style="text-align:right;">
0.7152622
</td>
</tr>
<tr>
<td style="text-align:left;">
Little Harbour
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
1.010101
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
0.3428571
</td>
<td style="text-align:right;">
0.1722624
</td>
<td style="text-align:right;">
1.660551
</td>
<td style="text-align:right;">
95.23452
</td>
<td style="text-align:right;">
99.42675
</td>
<td style="text-align:right;">
0.8283392
</td>
</tr>
<tr>
<td style="text-align:left;">
Marsh Harbour
</td>
<td style="text-align:right;">
0.7142857
</td>
<td style="text-align:right;">
1.006470
</td>
<td style="text-align:right;">
0.6666667
</td>
<td style="text-align:right;">
0.5166052
</td>
<td style="text-align:right;">
-0.3414096
</td>
<td style="text-align:right;">
1.448089
</td>
<td style="text-align:right;">
103.96229
</td>
<td style="text-align:right;">
153.76873
</td>
<td style="text-align:right;">
0.5430675
</td>
</tr>
<tr>
<td style="text-align:left;">
Mount Hope
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
1.176471
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
0.9448819
</td>
<td style="text-align:right;">
0.3921176
</td>
<td style="text-align:right;">
2.472720
</td>
<td style="text-align:right;">
104.99752
</td>
<td style="text-align:right;">
135.38070
</td>
<td style="text-align:right;">
0.4523715
</td>
</tr>
<tr>
<td style="text-align:left;">
Murphy Town
</td>
<td style="text-align:right;">
0.6666667
</td>
<td style="text-align:right;">
1.000000
</td>
<td style="text-align:right;">
0.6666667
</td>
<td style="text-align:right;">
0.4411765
</td>
<td style="text-align:right;">
-0.5236980
</td>
<td style="text-align:right;">
1.220285
</td>
<td style="text-align:right;">
89.89036
</td>
<td style="text-align:right;">
92.95444
</td>
<td style="text-align:right;">
0.7561668
</td>
</tr>
<tr>
<td style="text-align:left;">
Sandy Point
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
1.266501
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
0.8290816
</td>
<td style="text-align:right;">
0.1011147
</td>
<td style="text-align:right;">
2.337081
</td>
<td style="text-align:right;">
118.34135
</td>
<td style="text-align:right;">
277.19642
</td>
<td style="text-align:right;">
0.1900341
</td>
</tr>
<tr>
<td style="text-align:left;">
Spring City
</td>
<td style="text-align:right;">
0.8000000
</td>
<td style="text-align:right;">
1.000000
</td>
<td style="text-align:right;">
0.7500000
</td>
<td style="text-align:right;">
0.3703704
</td>
<td style="text-align:right;">
-0.2756593
</td>
<td style="text-align:right;">
1.360790
</td>
<td style="text-align:right;">
91.19001
</td>
<td style="text-align:right;">
96.13258
</td>
<td style="text-align:right;">
0.8155244
</td>
</tr>
<tr>
<td style="text-align:left;">
Treasure Cay
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
1.024765
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
0.4545455
</td>
<td style="text-align:right;">
-0.0502422
</td>
<td style="text-align:right;">
1.817121
</td>
<td style="text-align:right;">
100.95502
</td>
<td style="text-align:right;">
129.50441
</td>
<td style="text-align:right;">
0.7067973
</td>
</tr>
<tr>
<td style="text-align:left;">
Wood Cay
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
1.067616
</td>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
0.6000000
</td>
<td style="text-align:right;">
-0.1863227
</td>
<td style="text-align:right;">
1.793495
</td>
<td style="text-align:right;">
99.18593
</td>
<td style="text-align:right;">
118.39332
</td>
<td style="text-align:right;">
0.7110423
</td>
</tr>
</tbody>
</table>
<p>As we can see some of the potential aggregation algorithm are not providing results from some location. We will therefore exclude them from the rest of the analysis.</p>
<pre class="r"><code>## Eliminate automatically method that could not score some elements
this.indic.matrix.norm22 &lt;- this.indic.matrix.norm2[, colSums(this.indic.matrix.norm2 != 0, na.rm = TRUE) &gt; 0]

## Check if sum is  zero
this.indic.matrix.norm22 &lt;- this.indic.matrix.norm22[, colSums(this.indic.matrix.norm22 != 0, na.rm = TRUE)  == nrow(this.indic.matrix.norm22)]

## Remove indic if standard deviation is o
this.indic.matrix.norm22 &lt;- this.indic.matrix.norm22[, sapply(this.indic.matrix.norm22, function(x) { sd(x) != 0} )]

## Remove “NaN” or “Not a Number
this.indic.matrix.norm22 %&gt;%
  summarise_all(function(x) sum(x[!is.na(x)] == &quot;NaN&quot;) == length(x[!is.na(x)])) %&gt;% # check if number of NaN is equal to number of rows after removing NAs
  select_if(function(x) x == FALSE) %&gt;%       # select columns that don&#39;t have only nulls
  names() -&gt; vars_to_keep                     # keep column names

# select columns captured above
this.indic.matrix.norm22 &lt;- this.indic.matrix.norm22[ , vars_to_keep]                 



rm( ci_bod_constr_est_mpi, # Robust Benefit of the Doubt approach with constraint
  CI_BoD_MPI_estimated,
  ci_bod_est, # Benefit of the Doubt approach
  CI_BoD_estimated,
  ci_bod_dir_est, # Directional Robust Benefit of the Doubt approach
  CI_Direct_BoD_estimated,
  ci_rbod_est, # Robust Benefit of the Doubt approach
  CI_RBoD_estimated,
  ci_factor_est, # Factor analysis  componnents,
  dimfactor,
  CI_Factor_estimated,
  ci_mean_min_est, # Mean-Min Function
  CI_mean_min_estimated,
  ci_mpi_est, # Mazziotta-Pareto Index
  CI_MPI_estimated,
  ci_mean_geom_est, # Geometric aggregation
  CI_Geom_estimated,
  ci_wroclaw_est,# Wroclaw taxonomy method
  CI_wroclaw_estimated) </code></pre>
</div>
<div id="differences-between-algorithms" class="section level3">
<h3>Differences between algorithms</h3>
<p>The various Index can be normalised again on a 0 to 1 scale in order to be compared. A specific treatment if necessary for index based on Factor analysis</p>
<pre class="r"><code>## Directory of algorythms..
methodo &lt;- c(&quot;Benef_Doubt&quot;,
          &quot;Benef_Doubt_Rob&quot;,
          &quot;Benef_Doubt_Dir&quot;,
          &quot;Benef_Doubt_Cons&quot;,
          &quot;Factor&quot;,
          &quot;Mean_Geom&quot;,
          &quot;Mean_Min&quot;,
          &quot;Mazziotta_Pareto&quot;,
          &quot;Wroclaw&quot;)

label &lt;- c(    &quot;Benefit of the Doubt Approach&quot;,
               &quot;Robust Benefit of the Doubt Approach&quot;,
               &quot;Directional Robust Benefit of the Doubt Approach&quot;,
               &quot;Robust Benefit of the Doubt approach with constraint&quot;,
               &quot;Factor Analysis Componnents&quot;,
               &quot;Geometric Aggregation&quot;,
               &quot;Mean-Min Function&quot;,
               &quot;Mazziotta-Pareto Index&quot;,
               &quot;Wroclaw Taxonomy&quot;)
polarity &lt;- c( &quot;POS&quot;,
                &quot;POS&quot;,
                &quot;POS&quot;,
                &quot;POS&quot;,
                &quot;POS&quot;,
                &quot;POS&quot;,
                &quot;POS&quot;,
                &quot;POS&quot;,
                &quot;NEG&quot;)

All.method &lt;- as.data.frame( cbind(methodo, label,polarity))


## Factor analysis can provide negative rank - If it works we need to get rid of them
for (j in 1:nrow(this.indic.matrix.norm22)) {
this.indic.matrix.norm22[j ,c(&quot;Factor&quot;)] &lt;- ifelse( min(this.indic.matrix.norm22[ ,c(&quot;Factor&quot;)]) &lt; 0 ,
                                                    this.indic.matrix.norm22[j ,c(&quot;Factor&quot;)] + 
                                                      abs(min(this.indic.matrix.norm22[ ,c(&quot;Factor&quot;)])) ,
                                                     this.indic.matrix.norm22[j ,c(&quot;Factor&quot;)]) 
}

kept.methodo &lt;- as.data.frame(names(this.indic.matrix.norm22))
polarity2 &lt;- as.character(All.method[ All.method$methodo %in% names(this.indic.matrix.norm22),
                                      c(&quot;polarity&quot;) ])


this.indic.matrix.norm3 &lt;- normalise_ci(this.indic.matrix.norm22,
                                        c(1:ncol(this.indic.matrix.norm22)),
                                        polarity =  polarity2,
                                        method = 3)
this.indic.matrix.norm3 &lt;- this.indic.matrix.norm3$ci_norm

kable(this.indic.matrix.norm3, caption = &quot;Location Ranking with different algorithms&quot;) %&gt;%
           kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;bordered&quot;, &quot;condensed&quot;, &quot;responsive&quot;), font_size = 9)</code></pre>
<table class="table table-striped table-bordered table-condensed table-responsive" style="font-size: 9px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-20">Table 3: </span>Location Ranking with different algorithms
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Benef_Doubt
</th>
<th style="text-align:right;">
Benef_Doubt_Rob
</th>
<th style="text-align:right;">
Benef_Doubt_Dir
</th>
<th style="text-align:right;">
Benef_Doubt_Cons
</th>
<th style="text-align:right;">
Factor
</th>
<th style="text-align:right;">
Mean_Geom
</th>
<th style="text-align:right;">
Mean_Min
</th>
<th style="text-align:right;">
Mazziotta_Pareto
</th>
<th style="text-align:right;">
Wroclaw
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Bahama Palm Shores
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
20.0
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
23.0
</td>
<td style="text-align:right;">
23
</td>
<td style="text-align:right;">
23.0
</td>
<td style="text-align:right;">
20.0
</td>
<td style="text-align:right;">
16.0
</td>
<td style="text-align:right;">
22.0
</td>
</tr>
<tr>
<td style="text-align:left;">
Bahamas Coral Island
</td>
<td style="text-align:right;">
2.0
</td>
<td style="text-align:right;">
3.5
</td>
<td style="text-align:right;">
2.0
</td>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:right;">
1.0
</td>
</tr>
<tr>
<td style="text-align:left;">
Blackwood
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
22.0
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
18.0
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
21.5
</td>
<td style="text-align:right;">
18.0
</td>
<td style="text-align:right;">
19.0
</td>
<td style="text-align:right;">
16.0
</td>
</tr>
<tr>
<td style="text-align:left;">
Casuarina Point
</td>
<td style="text-align:right;">
7.0
</td>
<td style="text-align:right;">
15.0
</td>
<td style="text-align:right;">
7.0
</td>
<td style="text-align:right;">
16.0
</td>
<td style="text-align:right;">
21
</td>
<td style="text-align:right;">
16.0
</td>
<td style="text-align:right;">
15.0
</td>
<td style="text-align:right;">
12.0
</td>
<td style="text-align:right;">
20.5
</td>
</tr>
<tr>
<td style="text-align:left;">
Cedar Harbour
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
9.0
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
17.0
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
17.0
</td>
<td style="text-align:right;">
16.0
</td>
<td style="text-align:right;">
15.0
</td>
<td style="text-align:right;">
14.5
</td>
</tr>
<tr>
<td style="text-align:left;">
Central Pines
</td>
<td style="text-align:right;">
2.0
</td>
<td style="text-align:right;">
3.5
</td>
<td style="text-align:right;">
2.0
</td>
<td style="text-align:right;">
2.0
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
2.0
</td>
<td style="text-align:right;">
2.0
</td>
<td style="text-align:right;">
2.0
</td>
<td style="text-align:right;">
2.0
</td>
</tr>
<tr>
<td style="text-align:left;">
Cherokee Sound
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
16.0
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
20.0
</td>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
19.0
</td>
<td style="text-align:right;">
22.0
</td>
<td style="text-align:right;">
21.0
</td>
<td style="text-align:right;">
17.0
</td>
</tr>
<tr>
<td style="text-align:left;">
Cooper’s Town
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
10.0
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
13.0
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
13.0
</td>
<td style="text-align:right;">
12.0
</td>
<td style="text-align:right;">
18.0
</td>
<td style="text-align:right;">
11.0
</td>
</tr>
<tr>
<td style="text-align:left;">
Crossing Rocks
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
7.5
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
14.0
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
14.0
</td>
<td style="text-align:right;">
11.0
</td>
<td style="text-align:right;">
11.0
</td>
<td style="text-align:right;">
12.0
</td>
</tr>
<tr>
<td style="text-align:left;">
Crown Heaven
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
18.0
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
21.0
</td>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
20.0
</td>
<td style="text-align:right;">
21.0
</td>
<td style="text-align:right;">
20.0
</td>
<td style="text-align:right;">
19.0
</td>
</tr>
<tr>
<td style="text-align:left;">
Dundas Town
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
13.0
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
8.0
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
4.0
</td>
<td style="text-align:right;">
6.0
</td>
<td style="text-align:right;">
8.0
</td>
<td style="text-align:right;">
7.0
</td>
</tr>
<tr>
<td style="text-align:left;">
Fire Road
</td>
<td style="text-align:right;">
5.5
</td>
<td style="text-align:right;">
3.5
</td>
<td style="text-align:right;">
5.5
</td>
<td style="text-align:right;">
4.5
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
5.5
</td>
<td style="text-align:right;">
4.5
</td>
<td style="text-align:right;">
4.5
</td>
<td style="text-align:right;">
4.5
</td>
</tr>
<tr>
<td style="text-align:left;">
Fox Town
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
7.5
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
15.0
</td>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;">
15.0
</td>
<td style="text-align:right;">
14.0
</td>
<td style="text-align:right;">
14.0
</td>
<td style="text-align:right;">
14.5
</td>
</tr>
<tr>
<td style="text-align:left;">
Great Cistern
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
23.0
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
10.0
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
9.0
</td>
<td style="text-align:right;">
7.0
</td>
<td style="text-align:right;">
7.0
</td>
<td style="text-align:right;">
13.0
</td>
</tr>
<tr>
<td style="text-align:left;">
Leisure Lee
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
3.5
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
11.0
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
10.0
</td>
<td style="text-align:right;">
9.0
</td>
<td style="text-align:right;">
10.0
</td>
<td style="text-align:right;">
8.0
</td>
</tr>
<tr>
<td style="text-align:left;">
Little Harbour
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
12.0
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
3.0
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
8.0
</td>
<td style="text-align:right;">
8.0
</td>
<td style="text-align:right;">
6.0
</td>
<td style="text-align:right;">
3.0
</td>
</tr>
<tr>
<td style="text-align:left;">
Marsh Harbour
</td>
<td style="text-align:right;">
4.0
</td>
<td style="text-align:right;">
11.0
</td>
<td style="text-align:right;">
4.0
</td>
<td style="text-align:right;">
9.0
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
7.0
</td>
<td style="text-align:right;">
17.0
</td>
<td style="text-align:right;">
22.0
</td>
<td style="text-align:right;">
18.0
</td>
</tr>
<tr>
<td style="text-align:left;">
Mount Hope
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
19.0
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
22.0
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
21.5
</td>
<td style="text-align:right;">
19.0
</td>
<td style="text-align:right;">
17.0
</td>
<td style="text-align:right;">
20.5
</td>
</tr>
<tr>
<td style="text-align:left;">
Murphy Town
</td>
<td style="text-align:right;">
2.0
</td>
<td style="text-align:right;">
3.5
</td>
<td style="text-align:right;">
2.0
</td>
<td style="text-align:right;">
6.0
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3.0
</td>
<td style="text-align:right;">
3.0
</td>
<td style="text-align:right;">
3.0
</td>
<td style="text-align:right;">
6.0
</td>
</tr>
<tr>
<td style="text-align:left;">
Sandy Point
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
21.0
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
19.0
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
18.0
</td>
<td style="text-align:right;">
23.0
</td>
<td style="text-align:right;">
23.0
</td>
<td style="text-align:right;">
23.0
</td>
</tr>
<tr>
<td style="text-align:left;">
Spring City
</td>
<td style="text-align:right;">
5.5
</td>
<td style="text-align:right;">
3.5
</td>
<td style="text-align:right;">
5.5
</td>
<td style="text-align:right;">
4.5
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
5.5
</td>
<td style="text-align:right;">
4.5
</td>
<td style="text-align:right;">
4.5
</td>
<td style="text-align:right;">
4.5
</td>
</tr>
<tr>
<td style="text-align:left;">
Treasure Cay
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
14.0
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
7.0
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
12.0
</td>
<td style="text-align:right;">
13.0
</td>
<td style="text-align:right;">
13.0
</td>
<td style="text-align:right;">
10.0
</td>
</tr>
<tr>
<td style="text-align:left;">
Wood Cay
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
17.0
</td>
<td style="text-align:right;">
15.5
</td>
<td style="text-align:right;">
12.0
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
11.0
</td>
<td style="text-align:right;">
10.0
</td>
<td style="text-align:right;">
9.0
</td>
<td style="text-align:right;">
9.0
</td>
</tr>
</tbody>
</table>
<p>Let’s write this back to the excel doc</p>
<pre class="r"><code>datawrite &lt;- cbind(this.indic.df,
  this.indic.matrix.norm,
  this.indic.matrix.norm22,
  this.indic.matrix.norm3 )
wb &lt;- loadWorkbook(&quot;DTM R3 DB Great-Little Abaco MSLA V4.xlsx&quot;)
choicesSheet &lt;- xlsx::createSheet(wb, sheetName = this.dimension)
xlsx::addDataFrame(datawrite, choicesSheet, col.names = TRUE, row.names = TRUE)
xlsx::saveWorkbook(wb, &quot;DTM R3 DB Great-Little Abaco MSLA V5.xlsx&quot;)</code></pre>
<p>We can now build a visualization for the comparison between different valid methods.</p>
<pre class="r"><code>## Remove NaN
this.indic.matrix.norm4 &lt;-  this.indic.matrix.norm3[,colSums(this.indic.matrix.norm3 != 0, na.rm = TRUE) &gt; 0]

## keep that frame for later on for the viz
assign(  paste(&quot;scores.&quot;, this.dimension, sep = &quot;&quot;), this.indic.matrix.norm3 )

## Add blank variable for nice chart display
this.indic.matrix.norm4$Location &lt;- NA
  
this.indic.matrix.norm4.melt &lt;- melt(as.matrix(this.indic.matrix.norm4))


#Make plot
line &lt;- ggplot(this.indic.matrix.norm4.melt, aes(x = Var2,
                                                 y = value,
                                                 color = Var1,
                                                 group = Var1)) +
  geom_line(size = 2) +
  scale_colour_manual(values = c(&quot;#8dd3c7&quot;,&quot;#A6CEE3&quot;, &quot;#1F78B4&quot;, &quot;#B2DF8A&quot;, &quot;#33A02C&quot;,
                                 &quot;#FB9A99&quot;, &quot;#E31A1C&quot;, &quot;#FDBF6F&quot;, &quot;#FF7F00&quot;, &quot;#CAB2D6&quot;,
                                 &quot;#6A3D9A&quot;, &quot;#fb8072&quot;, &quot;#B15928&quot;, &quot;#fdb462&quot;,&quot;#ccebc5&quot;,
                                 &quot;#000000&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;,
                                 &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;)) + ## as many color as locations.. 23
  geom_text_repel(
    data = this.indic.matrix.norm4.melt[ this.indic.matrix.norm4.melt$Var2 == &quot;Wroclaw&quot;, ],
    aes(label = Var1),
    arrow = arrow(length = unit(0.03, &quot;npc&quot;), type = &quot;closed&quot;, ends = &quot;first&quot;),
    direction = &quot;y&quot;,
    size = 4,
    nudge_x = 45 ) +
  labs(title = paste0(&quot;Rank for Composite Indicator on &quot;,  this.dimension ),
       subtitle = &quot;Based on various weighting approach&quot;) +
  bbc_style() +
  theme( plot.title = element_text(size = 13),
         plot.subtitle = element_text(size = 11),
         plot.caption = element_text(size = 7, hjust = 1),
         axis.text = element_text(size = 10),
         strip.text.x = element_text(size = 11),
         panel.grid.major.x = element_line(color = &quot;#cbcbcb&quot;),
         panel.grid.major.y = element_blank(),
         axis.text.x = element_text(angle = 45, hjust = 1),
         legend.position = &quot;none&quot;)

print(ggpubr::ggarrange(left_align(line, c(&quot;subtitle&quot;, &quot;title&quot;)), ncol = 1, nrow = 1))</code></pre>
<p><img src="/post/CompositeIndicator_files/figure-html/unnamed-chunk-22-1.png" width="960" /></p>
</div>
<div id="index-sensitivity-to-method" class="section level3">
<h3>Index sensitivity to method</h3>
<p>As we can see, the final ranking for each location is very sensitive to the methods.</p>
<p>An approach to select the method can be to identify, average ranks per method in order to identify the method that is getting closer this average ranks.</p>
<p>we can first visualise those average ranks.</p>
<p><img src="/post/CompositeIndicator_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>Next is to compute standard deviation for each method.</p>
</div>
<div id="severity-index-on-a-map" class="section level3">
<h3>Severity Index on a map</h3>
<p>We can now visualise the thematic indicator on a map.</p>
<p>First, let’s get the background map from OSM. This requires a bit of testing to have the correct background aligned with the map bounding box.</p>
<pre class="r"><code>#plot(abaco)
# Create a boundary box
## https://www.openstreetmap.org/export#map=9/26.4226/-77.3259
longitudes &lt;- c(-78.3, -76.3)
latitudes &lt;- c(27.09, 25.75)
bounding_box &lt;- SpatialPointsDataFrame( as.data.frame(cbind(longitudes, latitudes)),
                                        as.data.frame(cbind(longitudes, latitudes)) , 
                                        proj4string = CRS(&quot;+init=epsg:4326&quot;) )

# download osm tiles
abaco.osm &lt;- getTiles(
   x = bounding_box,
   type = &quot;osm&quot;,
   zoom = 11,
   crop = TRUE,
   verbose = FALSE
  )</code></pre>
<pre><code>## Data and map tiles sources:
## © OpenStreetMap contributors. Tiles style under CC BY-SA, www.openstreetmap.org/copyright.</code></pre>
<pre class="r"><code>#tilesLayer(x = abaco.osm)</code></pre>
<p>Second step is to create a SpatialPointDataFrame with correct data and present it.</p>
<p>An initial visualisation will be to present both the severity and the population size affected by this severity.</p>
<pre class="r"><code>## Creating a spatial Point data frame with coordinates
abaco &lt;- SpatialPointsDataFrame(data[ ,c(&quot;C_103_lon&quot;,&quot;C_102_lat&quot;)], ## Coord
                                cbind(this.composite,
                                      data[ ,c(&quot;D_101_fams&quot;,&quot;D_102_inds&quot;,&quot;C_101_name&quot;)] ),  ## Composite with different algo - together with population
                                proj4string = CRS(&quot;+init=epsg:4326&quot;) ) ## Define projection syst 


# get Map Background
tilesLayer(x = abaco.osm)
#Plot symbols with choropleth coloration
propSymbolsChoroLayer(
  spdf = abaco,
  var = &quot;D_102_inds&quot;,
  inches = 0.25, ## size of the biggest symbol (radius for circles, width for squares, height for bars) in inches.
  border = &quot;grey50&quot;,
  lwd = 1, #width of symbols borders.
  legend.var.pos = &quot;topright&quot;,
  legend.var.title.txt = &quot;Population Size\n(# of Individual)&quot;,
  var2 = this.var ,
  #classification method; one of &quot;sd&quot;, &quot;equal&quot;, &quot;quantile&quot;, &quot;fisher-jenks&quot;,&quot;q6&quot;, &quot;geom&quot;, &quot;arith&quot;, &quot;em&quot; or &quot;msd&quot; 
  method = &quot;quantile&quot;,  
  nclass = 5,
  col = carto.pal(pal1 = &quot;sand.pal&quot;, n1 = 6),
  #legend.var2.values.rnd = -2,
  legend.var2.pos = &quot;left&quot;,
  legend.var2.title.txt = &quot;Index Value\n&quot;,
  legend.var.style = &quot;e&quot;
)
# Layout
layoutLayer(title = paste0( &quot;Severity  Index for &quot;, this.dimension, &quot; based on &quot;,this.label ),
            author = &quot;Protection Working Group, Abaco - The Bahamas, November 2019&quot;,
            sources = &quot;Source: Key Informant Interview - HDX/DTM/IOM&quot;, 
            tabtitle = TRUE, 
            frame = FALSE ,
            bg = &quot;#aad3df&quot;,
            scale = &quot;auto&quot;)
# North arrow
north(pos = &quot;topleft&quot;)</code></pre>
<p><img src="/post/CompositeIndicator_files/figure-html/unnamed-chunk-26-1.png" width="960" /></p>
<p>We can also try to present this information, using a smoothed map. For that we need to create first polygons based on points using the voronoi algorithm. Note that we use here the polygon mask for the island and that data are being re-projected (UTM-19N, i.e. epsg:32618) before producing the polygons.</p>
<pre class="r"><code>## Prepare maps...

abacomask &lt;- readOGR( paste0(getwd(),&quot;/abaco.geojson&quot;), verbose = FALSE)
abacomask &lt;- spTransform(abacomask, CRS(&quot;+init=epsg:32618&quot;))
            

## Create equivalent polygo with voronoi
# Using the union of points instead of the original sf object works:
voronoi &lt;- st_voronoi(st_union(st_as_sf(spTransform(abaco, CRS(&quot;+init=epsg:32618&quot;)))))
# Clid boundary instead of the original island boundaries
abacopoly &lt;- st_intersection(st_union(st_as_sf(abacomask)),
                             st_cast(voronoi))
abacopoly &lt;- as(abacopoly, &quot;Spatial&quot;)

rownames(abaco@data) &lt;- NULL
abaco@data$ID &lt;- rownames(abaco@data)
abacodata &lt;- as.data.frame(abaco@data)
rownames(abacodata) &lt;- paste0(&quot;ID&quot;,abaco$ID)
abacopoly &lt;- SpatialPolygonsDataFrame(abacopoly, abacodata)</code></pre>
<p>And now the smmoothed map - note the parameters: <code>typefct</code>, <code>span</code> and <code>beta</code> that requires a bit of testing for a correct visualisation.</p>
<p><img src="/post/CompositeIndicator_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
</div>
</div>

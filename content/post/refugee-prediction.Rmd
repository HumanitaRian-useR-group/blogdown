---
title: "Predicting Refugee Population with Panel Data Estimators and Gravity model"
author: "Chao Huang"
date: "2020-04-02"
categories:
  - Prediction
  - Refugee
tags:
  - UNHCR
  - Edouard-Legoupil
  - Chao-Huang
  - Andrea-Pellandra
---


To support programme development, UNHCR operations have to develop population planning figures. This is usually done through expert judgement. Though a predictive model, based historical data and pull/push indicators, can offer the advantage of cross checking expert appreciation.

The model presented in this tutorial is a [gravity model of migration](https://en.wikipedia.org/wiki/Gravity_model_of_migration), named because of the analogy with the Newtonian theory of gravitation, which assumes the movement between two countries is proportional to their size (population or GDP) and inversely proportional to the physical distance between them. It has long been popular for analyzing economic phenomena related to the movement of goods, service, capital or even people due to the extraordinary stability and its power to explain bilateral flows. 

With the collected data from 2000 to 2017, we predicted the refugee flows from any country of origin to any country of asylum in 2018 and 2019.

All data used in this tutorial are public open data:

- [Refugee data UNHCR Popstats](http://popstats.unhcr.org/en/overview)

- [GDP and Population data World Bank](https://data.worldbank.org/)

- [Complementary GDP data ](https://countryeconomy.com/)

- [Gravity related data CEPII (Centre d'Etudes Prospectives et d'Informations Internationales)](http://www.cepii.fr/CEPII/en/bdd_modele/presentation.asp?id=8)

- [Political stability and absence of violence World Bank](https://datacatalog.worldbank.org/political-stability-and-absence-violenceterrorism-estimate)

Other potential data sources, like the [ACLED - Armed Conflict Location & Event Data Project](https://acleddata.com), the [Uppsala Conflict Data Program](https://ucdp.uu.se/), [INFORM Global Risk Index](https://drmkc.jrc.ec.europa.eu/inform-index)) were initially tested but not included in the model due to  lack of data; bad performance in the modelling or hard to do the prediction.



```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
## Getting all necessary package
using <- function(...) {
  libs <- unlist(list(...))
  req <- unlist(lapply(libs,require,character.only = TRUE))
  need <- libs[req == FALSE]
  if (length(need) > 0) {
    install.packages(need, repos='https://mirrors.tuna.tsinghua.edu.cn/CRAN/')
    lapply(need,require,character.only = TRUE)
  }
}

using(
  ## Data
  'wbstats',
  'readxl',
  'data.table',
  'DT',
  'haven',
  ## Mapping
  'ISOcodes',
  'broom',
  'GGally','ggfortify','tseries','lmtest',# model diagnosis
  #plot
  'R.utils',
  'png',
  'grid',
  'ggplot2',
  'gganimate',
  'gghighlight',
  'ggpubr',
  'ggalt',
  'scales', # Scale Functions for Visualization
  'networkD3',
  'magrittr',
        
  # manipulation'tidyverse',
  'dplyr',
  'tidyr',
  'zoo',
  'xml2',
  'tidyverse',
  'forcats', # Tools for Working with Categorical Variables
  #time series
  'plm', # Linear Models for Panel Data
  'texreg'
)

unhcr_style <- function() {
  font <- "Lato"
  ggplot2::theme(

    #This sets the font, size, type and colour of text for the chart's title
    plot.title = ggplot2::element_text(family = font, size = 20, face = "bold", color = "#222222"),

    #This sets the font, size, type and colour of text for the chart's subtitle,  as well as setting a margin between the title and the subtitle
    plot.subtitle = ggplot2::element_text(family = font, size = 16, margin = ggplot2::margin(9,0,9,0)),
    plot.caption = ggplot2::element_blank(),

    #This sets the position and alignment of the legend, removes a title and backround for it and sets the requirements for any text within the legend. The legend may often need some more manual tweaking when it comes to its exact position based on the plot coordinates.
    legend.position = "top",
    legend.text.align = 0,
    legend.background = ggplot2::element_blank(),
    legend.title = ggplot2::element_blank(),
    legend.key = ggplot2::element_blank(),
    legend.text = ggplot2::element_text(family = font, size = 13, color = "#222222"),

    #This sets the text font, size and colour for the axis test, as well as setting the margins and removes lines and ticks. In some cases, axis lines and axis ticks are things we would want to have in the chart
    axis.title = ggplot2::element_blank(),
    axis.text = ggplot2::element_text(family = font, size = 13, color = "#222222"),
    axis.text.x = ggplot2::element_text(margin = ggplot2::margin(5, b = 10)),
    axis.ticks = ggplot2::element_blank(),
    axis.line = ggplot2::element_blank(),

    #This removes all minor gridlines and adds major y gridlines. In many cases you will want to change this to remove y gridlines and add x gridlines.
    panel.grid.minor = ggplot2::element_blank(),
    panel.grid.major.y = ggplot2::element_line(color = "#cbcbcb"),
    panel.grid.major.x = ggplot2::element_blank(),

    #This sets the panel background as blank, removing the standard grey ggplot background colour from the plot
    panel.background = ggplot2::element_blank(),

    #This sets the panel background for facet-wrapped plots to white, removing the standard grey ggplot background colour and sets the title size of the facet-wrap title to font size 22
    strip.background = ggplot2::element_rect(fill = "white"),
    strip.text = ggplot2::element_text(size  = 13,  hjust = 0)
  )
}

#Left align text
left_align <- function(plot_name, pieces){
  grob <- ggplot2::ggplotGrob(plot_name)
  n <- length(pieces)
  grob$layout$l[grob$layout$name %in% pieces] <- 2
  return(grob)
}


## a little help function to better format numbers
format_si <- function(...) {
  function(x) {
    limits <- c(1e-24, 1e-21, 1e-18, 1e-15, 1e-12,
                1e-9,  1e-6,  1e-3,  1e0,   1e3,
                1e6,   1e9,   1e12,  1e15,  1e18,
                1e21,  1e24)
    prefix <- c("y",   "z",   "a",   "f",   "p",
                "n",   "",   "m",   " ",   "k",
                "M",   "G",   "T",   "P",   "E",
                "Z",   "Y")

    # Vector with array indices according to position in intervals
    i <- findInterval(abs(x), limits)

    # Set prefix to " " for very small values < 1e-24
    i <- ifelse(i == 0, which(limits == 1e0), i)

    paste(format(round(x/limits[i], 1),
                 trim = TRUE, scientific = FALSE, ...),
          prefix[i])
  }
}
```

# Dataset used for Analysis 

The dataset includes the refugee populations from COO to COA across time, as well as push and pull factors like GDP, Population, Political Stability. Additionally, the generalized distance between the COO and COA consists of Geo-distance, if the pair of countries have contiguous border, do they have colonial relationship in history, do they use the same language, do they use common currency, do they have the same religious belief et al. 

Considering the value of some variables is relatively small (between 0 to 1), the variables with large values,like refugees, GDP, population, Geo-distance are transformed into log value for smoothing.

##  UNHCR Dataset

The whole dataset is Panel data (also known as longitudinal or cross-sectional time-series data). The behavior of individual are observed across time. In our case,

 * Individual: country pair of COO and COA, 
 * COO:  Country of Origin, 
 * COA: Country of Asylum, 
 * Time: year. 



```{r refugee, echo=TRUE, message=FALSE, warning=FALSE}
# REFUGEE DATA

url <- paste( 'http://popstats.unhcr.org/en/time_series.csv')
destfile = "unhcr_popstats_export_time_series_all_data.csv"
if (!file.exists(destfile)) {
  #setInternet2(TRUE)
  download.file(url ,destfile,method = "auto") }
time_series <- read.csv("unhcr_popstats_export_time_series_all_data.csv", stringsAsFactors = FALSE,
                        skip = 3, na.string = c("", "*"))
names(time_series)[2] <- "Country"

## Load geographic mapping reference 
reference <- read.csv("reference.csv")

### Rewrite country name in UNHCR dataset for further matching - country of asylum
time_series$ctryiso <- as.character(time_series$Country)
time_series$ctryiso[time_series$Country == "C\xf4te d'Ivoire"] <- "Côte d'Ivoire"
time_series$ctryiso[time_series$Country == "Cura\xe7ao"] <- "Curaçao"
time_series$ctryiso[time_series$Country == "Palestinian"] <- "State of Palestine"

code_d <- reference %>% 
  select(namepostat, iso_3) %>% 
  rename(country = namepostat,
         iso_d = iso_3)
time_series <- merge(x = time_series , by.x = "ctryiso", all.x = TRUE, y = code_d , by.y = "country" )
#View(unique(time_series[ is.na(time_series$iso_d), c("ctryiso")]))


### Rewrite country name in UNHCR dataset for further matching - country of origin
time_series$origin_iso <- as.character(time_series$Origin)
time_series$origin_iso[time_series$Origin == "C\xf4te d'Ivoire"] <- "Côte d'Ivoire"
time_series$origin_iso[time_series$Origin == "Cura\xe7ao"] <- "Curaçao"
time_series$origin_iso[time_series$Origin == "Palestinian"] <- "State of Palestine"


code_o <- reference %>% 
  select(namepostat,iso_3) %>% 
  rename(country = namepostat,
         iso_o = iso_3)
time_series <- merge(x = time_series , by.x = "origin_iso", all.x = TRUE, y = code_o , by.y = "country" )
#View(unique(time_series[ is.na(time_series$iso_o), c("origin_iso")]))


refugee <- time_series %>% 
           filter(Population.type == "Refugees (incl. refugee-like situations)") %>% 
           filter(Year > 2000) %>% 
           na.omit() %>%
           select(-Population.type,-Country,-Origin) %>% 
           rename(year = Year,
                  origin = origin_iso,
                  asylum = ctryiso,
                  refugee = Value) %>%
           select(year,
                  origin,
                  iso_o,
                  asylum,
                  iso_d,
                  refugee)

refugee$pair <- paste(refugee$iso_o, refugee$iso_d, sep = "")
refugee <- refugee[-(which(refugee$iso_d == refugee$iso_o)),]
refugee <- data.table(refugee)

refugee <- unique(refugee,by = c("pair","year"))


```




## World bank data

Political Stability and Absence of Violence/Terrorism: Estimate - Political Stability and Absence of Violence/Terrorism measures perceptions of the likelihood of political instability and/or politically-motivated violence, including terrorism. Estimate gives the country's score on the aggregate indicator, in units of a standard normal distribution, i.e. ranging from approximately -2.5 to 2.5.

 The Worldwide Governance Indicators (WGI) project reports aggregate and individual governance indicators for over 200 countries and territories over the period 1996–, for six dimensions of governance:

 *  Voice and Accountability
 *  Political Stability and Absence of Violence
 *  Government Effectiveness
 *  Regulatory Quality
 *  Rule of Law
 *  Control of Corruption

```{r echo=TRUE, message=FALSE, warning=FALSE}
#wb_data DATA
wb_data <- wbstats::wb( indicator = c("SP.POP.TOTL", ## Population
                             "NY.GDP.MKTP.CD",  ## GDP
                             "NY.GDP.PCAP.CD", ## GDP per capita
                             "NY.GNP.PCAP.CD", ## GNP per capita
                             "SI.POV.GINI", ## Gini Index
                             "PV.EST", # Political Stability and Absence of Violence/Terrorism
                             "VA.EST" , #    Voice and Accountability
                             "GE.EST" , # Government Effectiveness
                             "RQ.EST" , # Regulatory Quality
                             "RL.EST" , # Rule of Law
                             "CC.EST" #, # Control of Corruption
                             # "DT.ODA.DACD.AGPA.OCOM.CD", # Gross ODA aid disbursement for other commodity assistance, DAC donors total
                             # "AG.LND.PRCP.MM"
    ),
    startdate = 2000, enddate = 2018, return_wide = TRUE)

# Renaming variables for further matching
names(wb_data)[1] <- "iso"
names(wb_data)[2] <- "year"
wb_data$year <- as.integer(wb_data$year)
wb_data <-  wb_data %>%
            select(iso,year,SP.POP.TOTL,NY.GDP.MKTP.CD,PV.EST) %>%
            rename(pop = SP.POP.TOTL, gdp = NY.GDP.MKTP.CD, pv = PV.EST)

```

There are not official GDP data for Syria and North Korea in some years, so extra data are brought in manually from [country Economy website](https://countryeconomy.com/).

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Syria GDP ##############
SYR <- read.csv("SYR_GDP.csv")
SYR <- SYR[,-1]
YEAR <- c(2000:2018)
for (i in YEAR) {
  wb_data[which(wb_data$iso == "SYR" & wb_data$year == i),"gdp"] <- SYR[which(SYR$iso == "SYR" & SYR$year == i),"gdp"]
}
# North Korea GDP
PRK <- read.csv("North_korea_gdp.csv")
PRK <- PRK[,-1]
for (i in YEAR) {
  wb_data[which(wb_data$iso == "PRK" & wb_data$year == i),"gdp"] <- PRK[which(PRK$iso == "PRK" & PRK$year == i),"gdp"]
}

wbdata_o <- wb_data  %>% rename(iso_o = iso, pop_o = pop, gdp_o = gdp,pv_o = pv)
wbdata_d <- wb_data  %>% rename(iso_d = iso, pop_d = pop, gdp_d = gdp, pv_d = pv)
```

## GeoDist, CEPII GRAVITY DATA

[GeoDist](http://www.cepii.fr/CEPII/fr/bdd_modele/presentation.asp?id=6) database, produced by CEPII (Centre d'Études Prospectives et d'Informations Internationales), provides several geographical variables, in particular bilateral distances measured using city-level data to account for the geographic distribution of population inside each nation. Different measures of bilateral distances are available for 225 countries. For most of them, different calculations of "intra-national distances" are also available.

Some fix are required to account for certain countries.

```{r echo=TRUE, message=FALSE, warning=FALSE, , warning=FALSE, comment=NA}
# The Distance/If share contiguous border/Has ever Colonial/If use the same language, from 
url <- paste( 'http://www.cepii.fr/DATA_DOWNLOAD/gravity/gravdata_cepii.zip') 
destfile = "gravdata_cepii.zip" 
if (!file.exists(destfile)) {
    #setInternet2(TRUE)
    download.file(url ,destfile,method = "auto")
    unzip(destfile,exdir = getwd())  
    }

gravdata <- read_dta("gravdata.dta")

grav_data <- gravdata  %>% 
             filter(year == "2014") %>% 
             select(iso3_o, iso3_d, contig, comlang_ethno, col45, distw, comcur, comrelig) %>% 
             rename(iso_o = iso3_o,
                    iso_d = iso3_d,
                    comlang = comlang_ethno,
                    colony = col45)

## issue of SSD
ssd <- grav_data %>% 
       filter(iso_o == "SSD")

# use the data of SDN(sudan) to replace that of SSD
SDN_o <- grav_data %>% 
         filter(iso_o == "SDN")

SDN_d <- grav_data %>% 
         filter(iso_d == "SDN")

SDN_o[,1] <- "SSD"
SDN_d[,2] <- "SSD"

grav_data_SSD <- rbind(SDN_o, SDN_d)
grav_data <- rbind(grav_data, grav_data_SSD)

grav_data[which(grav_data$iso_o == "SSD" & grav_data$iso_d == "SDN"),3] <- 1
grav_data[which(grav_data$iso_o == "SSD" & grav_data$iso_d == "SDN"),4] <- 1
grav_data[which(grav_data$iso_o == "SSD" & grav_data$iso_d == "SDN"),5] <- 1
grav_data[which(grav_data$iso_d == "SSD" & grav_data$iso_o == "SDN"),3] <- 1
grav_data[which(grav_data$iso_d == "SSD" & grav_data$iso_o == "SDN"),4] <- 1
grav_data[which(grav_data$iso_d == "SSD" & grav_data$iso_o == "SDN"),5] <- 1

## issue of COD
COD_o <-  grav_data %>% 
          filter(iso_o == "ZAR") 

COD_d <-  grav_data %>% 
          filter(iso_d == "ZAR") 

COD_o[,1] <- "COD"
COD_d[,2] <- "COD"
grav_data_COD <- rbind(COD_o,COD_d)
grav_data <- rbind(grav_data,grav_data_COD)

## issue of Romania
grav_data[which(grav_data$iso_o == "ROM"),"iso_o"] <- "ROU"
grav_data[which(grav_data$iso_d == "ROM"),"iso_d"] <- "ROU"

##issue of Serbia

grav_data[which(grav_data$iso_o == "YUG"),"iso_o"] <- "SRB"
grav_data[which(grav_data$iso_d == "YUG"),"iso_d"] <- "SRB"

```

## Merging all data together

```{r echo=TRUE, message=FALSE, warning=FALSE}

refgrav <- left_join(refugee, grav_data, by = c("iso_o","iso_d"))
ref_grav_wb_o <- left_join(refgrav, wbdata_o, by = c("iso_o","year"))
ref_grav <- left_join(ref_grav_wb_o, wbdata_d, by = c("iso_d","year"))

ref_grav <- ref_grav %>%
            mutate(lref = log(refugee),
                     lndist = log(distw),
                     lpop_o = log(pop_o),
                     lpop_d = log(pop_d),
                     lgdp_o = log(gdp_o),
                     lgdp_d = log(gdp_d))
ref_grav <- pdata.frame(ref_grav,index = c("pair","year"))
rownames(ref_grav) <- 1:nrow(ref_grav)
rownames(ref_grav) <- NULL
```


# Modelling stage

## Dickey-Fuller test for stationary time series

Refugee populations are stock values. Ideally, the gravity model performs better with flow value. We could then compile the difference between the stock value would be one option. 

A Dickey-Fuller test can be used to check for stochastic trends and verify if the serie is stationary. A stationary series is one in which the properties – mean, variance and covariance, do not vary with time. For a series to be classified as stationary, it should not exhibit a trend.

If this is the case, and it is here, we can simply use the stock value (logarithm) to build the linear panel model with 'plm' package.

```{r echo=TRUE, message=FALSE, warning=FALSE, comment=""}
adf.test(ref_grav$refugee, k=2)
```


## Panel Data Estimators

Lets build first model of random effect.

```{r echo=TRUE, message=FALSE, warning=FALSE, , comment=""}

random <- plm( lref ~
               lag(lref) + # lref - log of refugee
               lndist +    # lndist - log of distance
               contig +    # contig - contiguous border or not
               comlang +   # conlang - common language
               colony +    # colony - colonial relationship in history
               comcur +    # comcur - common currency
               comrelig +  # comrelig - common religion
               lpop_o +    # lpop_o - log of population on country of origin
               lpop_d +    # lpop_d - log of population on country of asylum
               lgdp_o +    # lgdp_o - log of GDP on country of origin
               lgdp_d +    # lgdp_d - log of GDP on country of asylum
               pv_o +      # pv_o - Political Stability and Absence of Violence/Terrorism for country of origin
               pv_d,        # pv_d - Political Stability and Absence of Violence/Terrorism for country of asylum

            data = ref_grav,
            index = c("pair","year"),
            model = "random")

summary(random)
```
The results show most of the variables are significant, except for the common currency and population of destination country. This suggests that they are not the determinant factors for the movement of refugees. 

If we look at coefficients, we notice that some variables has __positive values__, such as  the refugees in last year, colonial relationship, common language, contiguous border, population of original country, GDP and stability of destination country have positive impact on the refugee flows, while others, like Geo-distance, religion belief, GDP and stability of original country are __negative__ to the movement. 

We can also present the model through a more visual forrest plot

```{r echo=TRUE, message=FALSE, warning=FALSE}

random.tidy <- broom::tidy(model.popgroup1a,
                                    conf.int = TRUE,
                                    conf.level = 0.95,
                                    exponentiate = TRUE
                                  )
#str(random.tidy)
random.tidy$fullname <- random.tidy$term
random.tidy <- stringdist_join(x = random.tidy, y = dico, by = "fullname", mode = "left", max_dist = 2)
random.tidy$term <- random.tidy$label
random.tidy <- random.tidy[ !(is.na(random.tidy$term)), ]



### Chart showing the model
#ggcoef(model.popgroup1a, 
plot1 <- ggcoef(random.tidy,        
       exponentiate = TRUE, color = "purple", shape = 18,  size = 3.5, exclude_intercept = TRUE, 
  vline_color = "red", vline_linetype =  "solid", errorbar_color = "black", errorbar_height = .25,
  conf.level = 0.95, conf.int = TRUE, 
  mapping = aes(x = estimate, y = term)) +
  xlab("Influence factor for each variable: Estimate of Odd Ratio") +
  ylab("") +
  labs(title = "Food Insecure",
           subtitle =  "Best combination of variables to explain this classification",
           caption = "The household possesing the characteristics on the left side are less likely to be in this group,\n while the one possesing the characteristics on the right side are more likely to in that group.\n The black lines (also called 'whiskers') around the point represent the confidence interval for each variable\n to be included in the model (the shorter the line, the more precise is the estimate)",
           x = NULL, y = NULL) +
    bbc_style() +
      theme( plot.title = element_text(size = 13),
             plot.subtitle = element_text(size = 11),
             plot.caption = element_text(size = 7, hjust = 1),
            axis.text = element_text(size = 9),
            panel.grid.major.x = element_line(color = "#cbcbcb"), 
            panel.grid.major.y = element_blank(),
            strip.text.x = element_text(size = 11))

ggpubr::ggarrange(left_align(plot1, c("subtitle", "title", "caption")), ncol = 1, nrow = 1)


ggcoef(random, exponentiate = TRUE, color = "purple", size = 5, shape = 18)

```

## Comparing models

Before we use the model to make the prediction, a question was raised as 'fixed effects'? To decide between fixed or random effects, we run a Hausman test where the null hypothesis is that the preferred model is random effects vs. the alternative the fixed effects (Green, 2008, chapter 9). The p-value is significant and indicates we should use fixed effect. However, an advantage of random effects is that you can include time-invariant variables (here distance, border, language, religion, and colonial relation). In the fixed effects model these variables are absorbed by the intercept. Another reason is the fixed effect will assign a coefficient for the time (year), which stands for some hidden factors not observable in that year. For training the model the results would be better. However, you cannot use it for the prediction, as you don't know the fixed effect in future.
```{r echo=TRUE, message=FALSE, warning=FALSE, comment=""}
# the fixed effects model
fix <- update(random, 
              model = "within")
# fix or random
phtest(fix, random)
```


'plm' provides various functions for estimation, like the first-difference model ("fd"), and the between model ("between"). The model estimation shows there is no great improvement for the random effect, so we are going to use the random model for prediction and accuracy test.

```{r echo=TRUE, message=FALSE, warning=FALSE, comment=""}
#the between model
bw <- update(random, 
             model = "between")

#the pooling model 
pool <-  update(random, 
             model = "pooling")

# Amemiya estimator for random effect
tways <- update(random,
                effect = "twoways",
                model = "random",
                random.method = "walhus")


screenreg( list("random" = random,
                "pooling" = pool,
                "fix" = fix,
                "between" = bw,
                "two ways" = tways),
           digits = 5)
```


## Testing accuracy

We divided the whole dataset into a training dataset and testing dataset. The lag and log function of plm are not always stable, so we are doing the logarithm and lag manually.

```{r  echo=TRUE, message=FALSE, warning=FALSE, comment=""}
namep <- row.names(random$model)
modelp <- ref_grav[row.names(ref_grav) %in% namep,]

modelp <- modelp %>%
          select(pair, year, origin, iso_o, asylum, iso_d, lref) %>%
          rename(logref = lref)

finaldata <- cbind(modelp,random$model)
finaldata$year <- as.integer(as.character(finaldata$year))

test.p <- finaldata %>% 
          filter(year > 2017) %>% 
          rename(lag.lref = "lag.lref.")

train.p <- finaldata %>% 
           filter(year<2018) %>% 
           rename(lag.lref="lag.lref.")

ran.p <- plm(lref ~ 
               lag.lref +  # lag.lref - log of refugee in last year
               lndist +    # lndist - log of distance
               contig +    # contig - contiguous border or not
               comlang +   # conlang - common language
               colony +    # colony - colonial relationship in history
               comcur +    # comcur - common currency
               comrelig +  # comrelig - common religion
               lpop_o +    # lpop_o - log of population on country of origin
               lpop_d +    # lpop_d - log of population on country of asylum
               lgdp_o +    # lgdp_o - log of GDP on country of origin
               lgdp_d +    # lgdp_d - log of GDP on country of asylum
               pv_o +      # pv_o - Political Stability and Absence of Violence/Terrorism for country of origin
               pv_d ,       # pv_d - Political Stability and Absence of Violence/Terrorism for country of asylum

           train.p,
           index = c("pair","year"),
           model = "random")

screenreg(list("Modeling variables" = ran.p),digits = 5)
```


Checking errors

```{r  echo=TRUE, message=FALSE, warning=FALSE, comment=""}
test.p$hat1 <- predict(ran.p,newdata = test.p)

test.p <- test.p %>%
          mutate(real = exp(lref),
                 esti1 = exp(hat1))

testp.error <- test.p %>% 
               transmute(year = year,
                         asylum = asylum,
                         iso_d = iso_d,
                         origin = origin,
                         iso_o = iso_o,
                         pair = pair,
                         real = real, 
                         esti1 = exp(hat1)) %>%
               mutate(error1 = (esti1 - real) / real)

mean(abs(testp.error$error1))
summary(abs(testp.error$error1))

allcountry <- testp.error %>%  
              group_by(year,asylum) %>% 
              summarise(real = sum(real),
                        esti1 = round(sum(esti1))) %>%
              mutate(error1 = (esti1 - real) / real)

mean(abs(allcountry$error1))
summary(abs(allcountry$error1))

```

Comparing with the official refugee data of 2018, the accuracy of the random effect model is 80%. 

The results for 2019 are also displayed but without verification, waiting for the official figures published in June of 2019. 

```{r  echo=TRUE, message=FALSE, warning=FALSE, comment=""}


test_2019 <- read.csv("test dataset for 2019 refugee.csv")
test_2019 <- test_2019[,-1]
test_2019 <- na.omit(test_2019)

test_2019$hat1 <- predict(ran.p,newdata = test_2019)

prediction_2019 <- test_2019 %>% 
                   mutate(esti1 = round(exp(hat1))) %>%
                   select(year,
                          origin,
                          iso_o,
                          asylum,
                          iso_d,
                          pair,esti1) %>% 
                  rename(pred = esti1)

allcountry2019 <- prediction_2019 %>%  
                  group_by(year,asylum) %>% 
                  summarise(pred = sum(pred))

```


# Visualising Output

Country shows the predicting results of the model in 2018 and 2019, from any country of origin to country of asylum. Overall values of refugee and estimation, together with average of absolute error, are shown and aligning with the chosen countries.

```{r  echo=TRUE, message=FALSE, warning=FALSE, comment=""}

p2018 <- testp.error %>% 
  select(year,origin,iso_o,asylum,iso_d,esti1,real,error1) %>% 
  rename(Year = year, 
         Origin = origin,
         Asylum = asylum,
         Estimation = esti1,
         Refugee = real,
         Error = error1)

p2019 <- prediction_2019 %>% 
  select(year,origin,iso_o,asylum,iso_d,pred) %>% 
  rename(Year = year, 
         Origin = origin,
         Asylum = asylum,
         Estimation = pred)

p2019$Refugee <- NA
p2019$Error <- NA

result <- rbind(p2018,p2019)

### Compile results per bureau

bureau_o <- reference %>% 
  select(iso_3, 
         main_office) %>% 
  rename(iso_o = iso_3,
         bureau_o = main_office)

bureau_d <- reference %>% 
  select(iso_3,main_office) %>% 
  rename(iso_d = iso_3,
         bureau_d = main_office)

region_bureau_o <- left_join(result, bureau_o, by = "iso_o")

region_bureau <- left_join(region_bureau_o, bureau_d, by = "iso_d")

region <- region_bureau %>% 
  group_by(Year,bureau_o,bureau_d) %>% 
  summarise( Estimation = sum(Estimation),
             Refugee = sum(Refugee)) %>%
  mutate(Error = (Estimation - Refugee) / Refugee) %>% 
  arrange(Year)

links1 <- readxl::read_excel("sankey_region.xlsx", sheet = "links")

nodes1 <-  readxl::read_excel("sankey_region.xlsx",sheet = "nodes")

links1$group <- as.factor(c("a","a","a","a","a","a","a",
                          "b","b","b","b","b","b","b",
                          "c","c","c","c","c","c","c",
                          "d","d","d",
                          "e","e","e","e","e","e","e",
                          "f","f","f","f","f","f","f",
                          "g","g","g","g","g","g","g"))
nodes1$group <- as.factor(c("my_unique_group"))

my_color <- 'd3.scaleOrdinal() .domain(["a", "b","c","d","e","f","g","my_unique_group"]) .range(["#74879F", "#338EC9", "#18375F","#FCF57F","#66D1C1","#F592A0","#F5C205","#CCCCCC"])'
```


```{r }
DT::datatable({result }, options = list(pageLength = 10)  )

```

Region sums up the results of the model in 2018 and 2019 at regional level.

```{r }
DT::datatable({region },  options = list(pageLength = 10) )

```
Region Viz displays the regional results in 2019 with a sankey chart, which shows the refugee movements are mainly within the regions, except for the flow from MENA to Europe.

Region Crossing displays the region-crossing results in 2019, that is - removing displacement within the regions. 

```{r echo=TRUE, fig.height=9, fig.width=9, message=FALSE, warning=FALSE}

sankeyNetwork(Links = links1,
              Nodes = nodes1,
              Source = "source",
              Target = "target",
              Value = "value",
              NodeID = "name",
              fontSize = 11,
              nodeWidth = 5,
              fontFamily = "sans-serif",
              iterations = 0,
              colourScale = my_color,
              LinkGroup = "group",
              NodeGroup = "group")

```

# Prediction for longer time

If you want to make a prediction for a longer time, like 2020 or 2021, you have to prepare the input data for the model manually, to be specific, prepare the data for time-variant variables, such as refugee in last year, GDP, population and political stability. And that's what we did for 2019. The estimation of the predictors is essential but also difficult to obtain, and there is no way to verify it. For instance, with the impact of Covid-19, the GDP prediction should be adjusted accordingly.



